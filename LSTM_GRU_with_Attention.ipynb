{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "roman-project",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Atchyuta\\\\Downloads'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-segment",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the the text files along with tag into CSV file.\n",
    "\n",
    "# Using readlines()\n",
    "file1 = open('./metadata/mapping_conv_topic.train.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    "import re \n",
    "count = 0\n",
    "# Strips the newline character\n",
    "final_list=[]\n",
    "for line in Lines:\n",
    "    count += 1\n",
    "    file_tag=re.findall('\"([^\"]*)\"', line)\n",
    "    file_no=line.split(' ')[0]\n",
    "    with open('trans.'+file_no+'.txt', 'r') as file:\n",
    "        file_data = file.read().replace('\\n', ' ')\n",
    "        file_data=re.sub(\"\\d+\\.\\d+ \", '',file_data)\n",
    "    final_list.append({\"file_name\":'trans.'+file_no+'.txt',\"file_content\":file_data,\"file_tag\":file_tag[0]})\n",
    "    #print(\"Line{}: {}\".format(count, line.strip()))\n",
    "\n",
    "import csv\n",
    "keys = final_list[0].keys()\n",
    "with open('train_data_tagging.csv', 'w', newline='')  as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(final_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "peripheral-painting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in d:\\anaconda3\\envs\\keras-tf\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in d:\\anaconda3\\envs\\keras-tf\\lib\\site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5.0 in d:\\anaconda3\\envs\\keras-tf\\lib\\site-packages (from gensim) (1.14.0)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.2.0.tar.gz (119 kB)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-4.2.0-py3-none-any.whl size=109632 sha256=353dfdabcc56405a11018dae28410c5e18beb5444d672f9f0a9097371c0f84ec\n",
      "  Stored in directory: c:\\users\\atchyuta\\appdata\\local\\pip\\cache\\wheels\\25\\88\\e3\\7cd51a6379cac37213cac47545a27688782752ff66351b953d\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "Successfully installed Cython-0.29.14 gensim-3.8.3 smart-open-4.2.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beautiful-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the file\n",
    "data=pd.read_csv('C:\\\\Users\\\\Atchyuta\\\\Downloads\\\\train_data_tagging_wo_duration.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "piano-transcription",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_content</th>\n",
       "      <th>file_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trans.2023.txt</td>\n",
       "      <td>Agent [silence] Customer all right Agent okay ...</td>\n",
       "      <td>Credit Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trans.2061.txt</td>\n",
       "      <td>Agent [noise] Customer [noise] Agent [silence]...</td>\n",
       "      <td>Credit Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trans.2067.txt</td>\n",
       "      <td>Agent [noise] Customer [noise] Agent well um w...</td>\n",
       "      <td>Credit Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trans.2163.txt</td>\n",
       "      <td>Agent [silence] Customer okay i was trying to ...</td>\n",
       "      <td>Credit Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trans.2313.txt</td>\n",
       "      <td>Agent [noise] Customer [silence] Agent [silenc...</td>\n",
       "      <td>Credit Card</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file_name                                       file_content  \\\n",
       "0  trans.2023.txt  Agent [silence] Customer all right Agent okay ...   \n",
       "1  trans.2061.txt  Agent [noise] Customer [noise] Agent [silence]...   \n",
       "2  trans.2067.txt  Agent [noise] Customer [noise] Agent well um w...   \n",
       "3  trans.2163.txt  Agent [silence] Customer okay i was trying to ...   \n",
       "4  trans.2313.txt  Agent [noise] Customer [silence] Agent [silenc...   \n",
       "\n",
       "      file_tag  \n",
       "0  Credit Card  \n",
       "1  Credit Card  \n",
       "2  Credit Card  \n",
       "3  Credit Card  \n",
       "4  Credit Card  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "romance-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "exposed-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import concatenate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk import word_tokenize          \n",
    "from nltk.corpus import stopwords\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Input, Bidirectional,TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "scientific-mystery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Credit Card', 'Bank Bailout', 'Taxes', 'Job Benefits',\n",
       "       'Family Finance', 'Budget'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['file_tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "strong-illinois",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "global-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing and cleaning Each sentence\n",
    "def buildVocabulary(sentence):\n",
    "    text = list(set(sentence))\n",
    "    tokenizer = Tokenizer(lower=False, split=' ')\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    return tokenizer\n",
    "def cleanSentence(sentence):\n",
    "     sentence_clean= re.sub(\"[^a-zA-Z0-9]\",\" \", str(sentence))\n",
    "     sentence_clean = sentence_clean.lower()\n",
    "     tokens = word_tokenize(sentence_clean)\n",
    "     stop_words = set(stopwords.words(\"english\"))\n",
    "     sentence_clean_words = [w for w in tokens if not w in stop_words]\n",
    "     return ' '.join(sentence_clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "balanced-length",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Atchyuta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Atchyuta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#clean the sentence \n",
    "data['clean_file_content'] = list(map(cleanSentence, data['file_content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beautiful-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text1 = '\\n'.join(data['clean_file_content'])\n",
    "sentences = corpus_text1.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "structured-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = buildVocabulary(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "familiar-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Tokenizer for Inference purpose\n",
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "other-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "accredited-observer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8216"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accepted-membership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.6'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "accompanied-wiring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_file_content</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>agent silence customer right agent okay custom...</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agent noise customer noise agent silence custo...</td>\n",
       "      <td>1676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agent noise customer noise agent well um credi...</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agent silence customer okay trying get childre...</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agent noise customer silence agent silence cus...</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>agent silence customer right agent well like t...</td>\n",
       "      <td>1314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agent noise customer noise agent silence custo...</td>\n",
       "      <td>1329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>agent silence customer noise customer okay um ...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>agent noise customer noise customer right agen...</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>agent silence customer noise customer probably...</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>agent noise customer noise agent silence custo...</td>\n",
       "      <td>1527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   clean_file_content  word_count\n",
       "0   agent silence customer right agent okay custom...        1320\n",
       "1   agent noise customer noise agent silence custo...        1676\n",
       "2   agent noise customer noise agent well um credi...         576\n",
       "3   agent silence customer okay trying get childre...         431\n",
       "4   agent noise customer silence agent silence cus...        1080\n",
       "5   agent silence customer right agent well like t...        1314\n",
       "6   agent noise customer noise agent silence custo...        1329\n",
       "7   agent silence customer noise customer okay um ...        1000\n",
       "8   agent noise customer noise customer right agen...         999\n",
       "9   agent silence customer noise customer probably...        1484\n",
       "10  agent noise customer noise agent silence custo...        1527"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['word_count'] = data['clean_file_content'].apply(lambda x: len(str(x).split(\" \")))\n",
    "#incidents_data['word_count'] = incidents_data['short_description'].apply(lambda x: len(str(x).split(\" \")))\n",
    "data[['clean_file_content','word_count']].head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "minor-affect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1676"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['word_count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "alternative-bouquet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "431"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['word_count'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "electrical-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Genrate Train Sequences\n",
    "def getTrainSequences(sentence, tokenizer,seq_maxlen):\n",
    "    sent1 = tokenizer.texts_to_sequences(sentence)\n",
    "    #sent_maxlen = max([len(s) for s in sent1])\n",
    "    #seq_maxlen = max([sent_maxlen])\n",
    "    return np.array(pad_sequences(sent1, maxlen=seq_maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "certified-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sequences for each sentence pair tuple\n",
    "train_data_need = getTrainSequences(sentences, tokenizer,seq_maxlen=1680)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "prescription-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 1680)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_need.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "original-dover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 8, 1, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_need[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "certified-triumph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x1e7c921f940>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rough-jewel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8215\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "important-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed_size = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "novel-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "#100d Glove vector file\n",
    "glove_file = 'C:\\\\Users\\\\Atchyuta\\\\Downloads\\\\glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "basic-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Glove file\n",
    "def loadGloveWordEmbeddings(glove_file):\n",
    "    embedding_vectors = {}\n",
    "    f = open(glove_file,encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        value = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_vectors[word] = value\n",
    "    f.close()\n",
    "    return embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "handmade-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating embedding matrix for each word based on Glove vector\n",
    "def getEmbeddingWeightMatrix(embedding_vectors, word2idx):    \n",
    "    embedding_matrix = np.zeros((len(word2idx)+1, word_embed_size))\n",
    "    for word, i in word2idx.items():\n",
    "        embedding_vector = embedding_vectors.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ranking-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = loadGloveWordEmbeddings(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "catholic-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('embedding_vectors.pickle', 'wb') as handle:\n",
    "    pickle.dump(embedding_vectors, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "loving-driver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "print(len(embedding_vectors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "documentary-creator",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent': 1,\n",
       " 'customer': 2,\n",
       " 'silence': 3,\n",
       " 'uh': 4,\n",
       " 'know': 5,\n",
       " 'yeah': 6,\n",
       " 'um': 7,\n",
       " 'laughter': 8,\n",
       " 'noise': 9,\n",
       " 'well': 10,\n",
       " 'like': 11,\n",
       " 'right': 12,\n",
       " 'think': 13,\n",
       " 'hum': 14,\n",
       " 'huh': 15,\n",
       " 'get': 16,\n",
       " 'oh': 17,\n",
       " 'really': 18,\n",
       " 'one': 19,\n",
       " 'money': 20,\n",
       " 'pay': 21,\n",
       " 'got': 22,\n",
       " 'people': 23,\n",
       " 'going': 24,\n",
       " 'lot': 25,\n",
       " 'mean': 26,\n",
       " 'much': 27,\n",
       " 'things': 28,\n",
       " 'would': 29,\n",
       " 'good': 30,\n",
       " 'go': 31,\n",
       " 'something': 32,\n",
       " 'kind': 33,\n",
       " 'credit': 34,\n",
       " '1': 35,\n",
       " 'time': 36,\n",
       " 'work': 37,\n",
       " 'okay': 38,\n",
       " 'thing': 39,\n",
       " 'guess': 40,\n",
       " 'tax': 41,\n",
       " 'see': 42,\n",
       " 'vocalized': 43,\n",
       " 'way': 44,\n",
       " 'taxes': 45,\n",
       " 'dollars': 46,\n",
       " 'use': 47,\n",
       " 'year': 48,\n",
       " 'say': 49,\n",
       " 'years': 50,\n",
       " 'two': 51,\n",
       " 'budget': 52,\n",
       " 'little': 53,\n",
       " 'card': 54,\n",
       " 'month': 55,\n",
       " 'yes': 56,\n",
       " 'back': 57,\n",
       " 'pretty': 58,\n",
       " 'even': 59,\n",
       " 'take': 60,\n",
       " 'make': 61,\n",
       " 'every': 62,\n",
       " 'could': 63,\n",
       " 'us': 64,\n",
       " 'need': 65,\n",
       " 'benefits': 66,\n",
       " 'sure': 67,\n",
       " 'want': 68,\n",
       " 'put': 69,\n",
       " 'five': 70,\n",
       " 'company': 71,\n",
       " 'probably': 72,\n",
       " 'paying': 73,\n",
       " 'anything': 74,\n",
       " 'cards': 75,\n",
       " 'percent': 76,\n",
       " 'hundred': 77,\n",
       " 'never': 78,\n",
       " 'said': 79,\n",
       " 'always': 80,\n",
       " 'insurance': 81,\n",
       " 'keep': 82,\n",
       " 'real': 83,\n",
       " 'state': 84,\n",
       " 'government': 85,\n",
       " 'still': 86,\n",
       " 'getting': 87,\n",
       " 'long': 88,\n",
       " 'paid': 89,\n",
       " 'three': 90,\n",
       " 'spend': 91,\n",
       " 'actually': 92,\n",
       " 'big': 93,\n",
       " 'income': 94,\n",
       " 'come': 95,\n",
       " 'stuff': 96,\n",
       " 'try': 97,\n",
       " 'many': 98,\n",
       " 'everything': 99,\n",
       " 'job': 100,\n",
       " 'used': 101,\n",
       " 'interest': 102,\n",
       " 'sort': 103,\n",
       " 'maybe': 104,\n",
       " 'school': 105,\n",
       " 'also': 106,\n",
       " 'feel': 107,\n",
       " 'went': 108,\n",
       " 'bit': 109,\n",
       " 'buy': 110,\n",
       " 'seems': 111,\n",
       " 'enough': 112,\n",
       " 'day': 113,\n",
       " 'course': 114,\n",
       " 'look': 115,\n",
       " 'na': 116,\n",
       " 'nice': 117,\n",
       " 'twenty': 118,\n",
       " 'amount': 119,\n",
       " 'let': 120,\n",
       " 'better': 121,\n",
       " 'far': 122,\n",
       " 'true': 123,\n",
       " 'savings': 124,\n",
       " 'first': 125,\n",
       " 'give': 126,\n",
       " 'care': 127,\n",
       " 'able': 128,\n",
       " 'last': 129,\n",
       " 'trying': 130,\n",
       " 'around': 131,\n",
       " 'car': 132,\n",
       " 'house': 133,\n",
       " 'another': 134,\n",
       " 'whatever': 135,\n",
       " 'problem': 136,\n",
       " 'thousand': 137,\n",
       " 'end': 138,\n",
       " 'though': 139,\n",
       " 'husband': 140,\n",
       " 'cash': 141,\n",
       " 'part': 142,\n",
       " 'health': 143,\n",
       " 'check': 144,\n",
       " 'else': 145,\n",
       " 'four': 146,\n",
       " 'talking': 147,\n",
       " 'ti': 148,\n",
       " 'exactly': 149,\n",
       " 'bad': 150,\n",
       " 'hard': 151,\n",
       " 'great': 152,\n",
       " 'new': 153,\n",
       " 'whole': 154,\n",
       " 'cut': 155,\n",
       " 'start': 156,\n",
       " 'gon': 157,\n",
       " 'working': 158,\n",
       " 'away': 159,\n",
       " 'ten': 160,\n",
       " 'week': 161,\n",
       " 'goes': 162,\n",
       " 'different': 163,\n",
       " 'home': 164,\n",
       " 'account': 165,\n",
       " 'plan': 166,\n",
       " 'find': 167,\n",
       " 'family': 168,\n",
       " 'college': 169,\n",
       " 'sometimes': 170,\n",
       " 'charge': 171,\n",
       " 'kids': 172,\n",
       " 'least': 173,\n",
       " 'quite': 174,\n",
       " 'bills': 175,\n",
       " 'important': 176,\n",
       " 'either': 177,\n",
       " 'live': 178,\n",
       " 'done': 179,\n",
       " 'high': 180,\n",
       " 'spending': 181,\n",
       " 'business': 182,\n",
       " 'thought': 183,\n",
       " 'months': 184,\n",
       " 'texas': 185,\n",
       " 'anyway': 186,\n",
       " 'ever': 187,\n",
       " 'dollar': 188,\n",
       " 'bye': 189,\n",
       " 'fifty': 190,\n",
       " 'call': 191,\n",
       " 'point': 192,\n",
       " 'federal': 193,\n",
       " 'made': 194,\n",
       " 'works': 195,\n",
       " 'times': 196,\n",
       " 'worked': 197,\n",
       " 'debt': 198,\n",
       " 'bill': 199,\n",
       " 'tell': 200,\n",
       " 'might': 201,\n",
       " 'couple': 202,\n",
       " 'ago': 203,\n",
       " 'everybody': 204,\n",
       " 'saying': 205,\n",
       " 'six': 206,\n",
       " 'days': 207,\n",
       " 'th': 208,\n",
       " 'thirty': 209,\n",
       " 'vacation': 210,\n",
       " 'somebody': 211,\n",
       " 'may': 212,\n",
       " 'companies': 213,\n",
       " 'since': 214,\n",
       " 'salary': 215,\n",
       " 'comes': 216,\n",
       " 'less': 217,\n",
       " 'especially': 218,\n",
       " 'idea': 219,\n",
       " 'interesting': 220,\n",
       " 'gets': 221,\n",
       " 'agree': 222,\n",
       " 'fact': 223,\n",
       " 'wife': 224,\n",
       " 'started': 225,\n",
       " 'wow': 226,\n",
       " 'medical': 227,\n",
       " 'system': 228,\n",
       " 'certain': 229,\n",
       " 'children': 230,\n",
       " 'basically': 231,\n",
       " 'talk': 232,\n",
       " 'large': 233,\n",
       " 'set': 234,\n",
       " 'sales': 235,\n",
       " 'almost': 236,\n",
       " 'cost': 237,\n",
       " 'area': 238,\n",
       " 'term': 239,\n",
       " 'help': 240,\n",
       " 'life': 241,\n",
       " 'spent': 242,\n",
       " 'place': 243,\n",
       " 'weeks': 244,\n",
       " 'save': 245,\n",
       " 'w': 246,\n",
       " 'ones': 247,\n",
       " 'country': 248,\n",
       " 'usually': 249,\n",
       " 'eight': 250,\n",
       " 'american': 251,\n",
       " 'seem': 252,\n",
       " 'visa': 253,\n",
       " 'type': 254,\n",
       " 'expenses': 255,\n",
       " 'half': 256,\n",
       " 'bank': 257,\n",
       " 'making': 258,\n",
       " 'married': 259,\n",
       " 'small': 260,\n",
       " 'situation': 261,\n",
       " 'benefit': 262,\n",
       " 'whether': 263,\n",
       " 'ou': 264,\n",
       " 'ahead': 265,\n",
       " 'easy': 266,\n",
       " 'next': 267,\n",
       " 'write': 268,\n",
       " 'makes': 269,\n",
       " 'left': 270,\n",
       " 'extra': 271,\n",
       " 'old': 272,\n",
       " 'e': 273,\n",
       " 'retirement': 274,\n",
       " 'worth': 275,\n",
       " 'happen': 276,\n",
       " 'gone': 277,\n",
       " 'absolutely': 278,\n",
       " 'yet': 279,\n",
       " 'heard': 280,\n",
       " 'number': 281,\n",
       " 'dental': 282,\n",
       " 'believe': 283,\n",
       " 'yep': 284,\n",
       " 'loan': 285,\n",
       " 'without': 286,\n",
       " 'took': 287,\n",
       " 'understand': 288,\n",
       " 'rather': 289,\n",
       " 'mine': 290,\n",
       " 'hm': 291,\n",
       " 'found': 292,\n",
       " 'seven': 293,\n",
       " 'person': 294,\n",
       " 'fifteen': 295,\n",
       " 'major': 296,\n",
       " 'expensive': 297,\n",
       " 'run': 298,\n",
       " 'looking': 299,\n",
       " 'computer': 300,\n",
       " 'gotten': 301,\n",
       " 'monthly': 302,\n",
       " 'sounds': 303,\n",
       " 'came': 304,\n",
       " 'coming': 305,\n",
       " 'programs': 306,\n",
       " 'problems': 307,\n",
       " 'bought': 308,\n",
       " 'gas': 309,\n",
       " 'sense': 310,\n",
       " 'world': 311,\n",
       " 'aside': 312,\n",
       " 'already': 313,\n",
       " 'best': 314,\n",
       " 'taking': 315,\n",
       " 'control': 316,\n",
       " 'payment': 317,\n",
       " 'wanted': 318,\n",
       " 'living': 319,\n",
       " 'loans': 320,\n",
       " 'security': 321,\n",
       " 'rate': 322,\n",
       " 'higher': 323,\n",
       " 'full': 324,\n",
       " 'property': 325,\n",
       " 'checks': 326,\n",
       " 'someone': 327,\n",
       " 'n': 328,\n",
       " 'mind': 329,\n",
       " 'christmas': 330,\n",
       " 'figure': 331,\n",
       " 'jobs': 332,\n",
       " 'places': 333,\n",
       " 'change': 334,\n",
       " 'express': 335,\n",
       " 'within': 336,\n",
       " 'remember': 337,\n",
       " 'trouble': 338,\n",
       " 'union': 339,\n",
       " 'balance': 340,\n",
       " 'deal': 341,\n",
       " 'economy': 342,\n",
       " 'city': 343,\n",
       " 'costs': 344,\n",
       " 'per': 345,\n",
       " 'stay': 346,\n",
       " 'raise': 347,\n",
       " 'personal': 348,\n",
       " 'instead': 349,\n",
       " 'reason': 350,\n",
       " 'takes': 351,\n",
       " 'definitely': 352,\n",
       " 'dallas': 353,\n",
       " 'doctor': 354,\n",
       " 'today': 355,\n",
       " 'education': 356,\n",
       " 'program': 357,\n",
       " 'hand': 358,\n",
       " 'matter': 359,\n",
       " 'using': 360,\n",
       " 'free': 361,\n",
       " 'although': 362,\n",
       " 'case': 363,\n",
       " 'forty': 364,\n",
       " 'line': 365,\n",
       " 'fee': 366,\n",
       " 'supposed': 367,\n",
       " 'anymore': 368,\n",
       " 'budgeting': 369,\n",
       " 'called': 370,\n",
       " 'ah': 371,\n",
       " 'buying': 372,\n",
       " 'certainly': 373,\n",
       " 'eighty': 374,\n",
       " 'hear': 375,\n",
       " 'send': 376,\n",
       " 'terms': 377,\n",
       " 'financial': 378,\n",
       " 'thinking': 379,\n",
       " 'schools': 380,\n",
       " 'student': 381,\n",
       " 'nine': 382,\n",
       " 'fund': 383,\n",
       " 'social': 384,\n",
       " 'unless': 385,\n",
       " 'realize': 386,\n",
       " 'plus': 387,\n",
       " 'afford': 388,\n",
       " 'services': 389,\n",
       " 'national': 390,\n",
       " 'difference': 391,\n",
       " 'eighteen': 392,\n",
       " 'nothing': 393,\n",
       " 'seen': 394,\n",
       " 'food': 395,\n",
       " 'payments': 396,\n",
       " 'wish': 397,\n",
       " 'short': 398,\n",
       " 'guy': 399,\n",
       " 'countries': 400,\n",
       " 'single': 401,\n",
       " 'defense': 402,\n",
       " 'paper': 403,\n",
       " 'fairly': 404,\n",
       " 'topic': 405,\n",
       " 'fair': 406,\n",
       " 'groceries': 407,\n",
       " 'rates': 408,\n",
       " 'close': 409,\n",
       " 'cover': 410,\n",
       " 'wh': 411,\n",
       " 'saving': 412,\n",
       " 'congress': 413,\n",
       " 'hey': 414,\n",
       " 'anybody': 415,\n",
       " 'leave': 416,\n",
       " 'office': 417,\n",
       " 'limit': 418,\n",
       " 'discover': 419,\n",
       " 'somewhere': 420,\n",
       " 'deficit': 421,\n",
       " 'carry': 422,\n",
       " 'needs': 423,\n",
       " 'covered': 424,\n",
       " 'happens': 425,\n",
       " 'often': 426,\n",
       " 'boy': 427,\n",
       " 'ought': 428,\n",
       " 'happened': 429,\n",
       " 'level': 430,\n",
       " 'several': 431,\n",
       " 'rid': 432,\n",
       " 'local': 433,\n",
       " 'moved': 434,\n",
       " 'paycheck': 435,\n",
       " 'pays': 436,\n",
       " 'starting': 437,\n",
       " 'hope': 438,\n",
       " 'putting': 439,\n",
       " 'low': 440,\n",
       " 'percentage': 441,\n",
       " 'checking': 442,\n",
       " 'taken': 443,\n",
       " 'mortgage': 444,\n",
       " 'sit': 445,\n",
       " 'sixty': 446,\n",
       " 'tried': 447,\n",
       " 'ha': 448,\n",
       " 'plans': 449,\n",
       " 'banks': 450,\n",
       " 'taxed': 451,\n",
       " 'twelve': 452,\n",
       " 'running': 453,\n",
       " 'general': 454,\n",
       " 'concerned': 455,\n",
       " 'biggest': 456,\n",
       " 'turn': 457,\n",
       " 'recently': 458,\n",
       " 'lower': 459,\n",
       " 'c': 460,\n",
       " 'means': 461,\n",
       " 'second': 462,\n",
       " 'tough': 463,\n",
       " 'states': 464,\n",
       " 'period': 465,\n",
       " 'kinds': 466,\n",
       " 'h': 467,\n",
       " 'r': 468,\n",
       " 'town': 469,\n",
       " 'seventy': 470,\n",
       " 'based': 471,\n",
       " 'add': 472,\n",
       " 'gosh': 473,\n",
       " 'ninety': 474,\n",
       " 'middle': 475,\n",
       " 'ways': 476,\n",
       " 'must': 477,\n",
       " 'wonderful': 478,\n",
       " 'god': 479,\n",
       " 'past': 480,\n",
       " 'rest': 481,\n",
       " 'price': 482,\n",
       " 'gave': 483,\n",
       " 'top': 484,\n",
       " 'keeping': 485,\n",
       " 'military': 486,\n",
       " 'university': 487,\n",
       " 'sell': 488,\n",
       " 'knew': 489,\n",
       " 'order': 490,\n",
       " 'track': 491,\n",
       " 'waste': 492,\n",
       " 'pick': 493,\n",
       " 'wan': 494,\n",
       " 'age': 495,\n",
       " 'together': 496,\n",
       " 'hours': 497,\n",
       " 'daughter': 498,\n",
       " 'question': 499,\n",
       " 'given': 500,\n",
       " 'department': 501,\n",
       " 'f': 502,\n",
       " 'totally': 503,\n",
       " 'says': 504,\n",
       " 'lose': 505,\n",
       " 'service': 506,\n",
       " 'hmo': 507,\n",
       " 'market': 508,\n",
       " 'b': 509,\n",
       " 'degree': 510,\n",
       " 'bring': 511,\n",
       " 'nd': 512,\n",
       " 'public': 513,\n",
       " 'lived': 514,\n",
       " 'longer': 515,\n",
       " 'coverage': 516,\n",
       " 'cars': 517,\n",
       " 'employees': 518,\n",
       " 'organization': 519,\n",
       " 'bucks': 520,\n",
       " 'p': 521,\n",
       " 'everyone': 522,\n",
       " 'cutting': 523,\n",
       " 'wonder': 524,\n",
       " 'sick': 525,\n",
       " 'child': 526,\n",
       " 'policy': 527,\n",
       " 'hit': 528,\n",
       " 'consider': 529,\n",
       " 'sorry': 530,\n",
       " 'particular': 531,\n",
       " 'sale': 532,\n",
       " 'man': 533,\n",
       " 'president': 534,\n",
       " 'helps': 535,\n",
       " 'kept': 536,\n",
       " 'later': 537,\n",
       " 'nineteen': 538,\n",
       " 'felt': 539,\n",
       " 'guys': 540,\n",
       " 'funny': 541,\n",
       " 'wait': 542,\n",
       " 'minimum': 543,\n",
       " 'sent': 544,\n",
       " 'store': 545,\n",
       " 'mail': 546,\n",
       " 'mastercard': 547,\n",
       " 'awful': 548,\n",
       " 'welfare': 549,\n",
       " 'stick': 550,\n",
       " 'wants': 551,\n",
       " 'easier': 552,\n",
       " 'california': 553,\n",
       " 'build': 554,\n",
       " 'whenever': 555,\n",
       " 'lost': 556,\n",
       " 'twice': 557,\n",
       " 'instance': 558,\n",
       " 'systems': 559,\n",
       " 'purchase': 560,\n",
       " 'involved': 561,\n",
       " 'handle': 562,\n",
       " 'basic': 563,\n",
       " 'besides': 564,\n",
       " 'doctors': 565,\n",
       " 'giving': 566,\n",
       " 'older': 567,\n",
       " 'profit': 568,\n",
       " 'pension': 569,\n",
       " 'research': 570,\n",
       " 'york': 571,\n",
       " 'generally': 572,\n",
       " 'industry': 573,\n",
       " 'worry': 574,\n",
       " 'name': 575,\n",
       " 'charged': 576,\n",
       " 'value': 577,\n",
       " 'dad': 578,\n",
       " 'advantage': 579,\n",
       " 'private': 580,\n",
       " 'forget': 581,\n",
       " 'except': 582,\n",
       " 'mother': 583,\n",
       " 'move': 584,\n",
       " 'clothes': 585,\n",
       " 'third': 586,\n",
       " 'se': 587,\n",
       " 'l': 588,\n",
       " 'retire': 589,\n",
       " 'crazy': 590,\n",
       " 'u': 591,\n",
       " 'travel': 592,\n",
       " 'x': 593,\n",
       " 'funds': 594,\n",
       " 'cents': 595,\n",
       " 'worse': 596,\n",
       " 'tend': 597,\n",
       " 'million': 598,\n",
       " 'special': 599,\n",
       " 'night': 600,\n",
       " 'hate': 601,\n",
       " 'regular': 602,\n",
       " 'feeling': 603,\n",
       " 'bigger': 604,\n",
       " 'soon': 605,\n",
       " 'young': 606,\n",
       " 'billion': 607,\n",
       " 'break': 608,\n",
       " 'subject': 609,\n",
       " 'annual': 610,\n",
       " 'imagine': 611,\n",
       " 'opportunity': 612,\n",
       " 'read': 613,\n",
       " 'class': 614,\n",
       " 'ready': 615,\n",
       " 'looked': 616,\n",
       " 'fun': 617,\n",
       " 'friends': 618,\n",
       " 'position': 619,\n",
       " 'outside': 620,\n",
       " 'willing': 621,\n",
       " 'suppose': 622,\n",
       " 'along': 623,\n",
       " 'support': 624,\n",
       " 'interested': 625,\n",
       " 'future': 626,\n",
       " 'average': 627,\n",
       " 'eventually': 628,\n",
       " 'rent': 629,\n",
       " 'needed': 630,\n",
       " 'return': 631,\n",
       " 'investment': 632,\n",
       " 'poor': 633,\n",
       " 'increase': 634,\n",
       " 'early': 635,\n",
       " 'hold': 636,\n",
       " 'planning': 637,\n",
       " 'terrible': 638,\n",
       " 'homes': 639,\n",
       " 'stop': 640,\n",
       " 'example': 641,\n",
       " 'keeps': 642,\n",
       " 'finance': 643,\n",
       " 'learn': 644,\n",
       " 'building': 645,\n",
       " 'decided': 646,\n",
       " 'en': 647,\n",
       " 'graduate': 648,\n",
       " 'show': 649,\n",
       " 'parents': 650,\n",
       " 'difficult': 651,\n",
       " 'deductible': 652,\n",
       " 'watch': 653,\n",
       " 'glad': 654,\n",
       " 'beginning': 655,\n",
       " 'saw': 656,\n",
       " 'areas': 657,\n",
       " 'necessarily': 658,\n",
       " 'summer': 659,\n",
       " 'lucky': 660,\n",
       " 'book': 661,\n",
       " 'accounts': 662,\n",
       " 'drive': 663,\n",
       " 'towards': 664,\n",
       " 'responsible': 665,\n",
       " 'bunch': 666,\n",
       " 'quit': 667,\n",
       " 'decide': 668,\n",
       " 'k': 669,\n",
       " 'deductions': 670,\n",
       " 'others': 671,\n",
       " 'hour': 672,\n",
       " 'scary': 673,\n",
       " 'prices': 674,\n",
       " 'ridiculous': 675,\n",
       " 'machine': 676,\n",
       " 'g': 677,\n",
       " 'group': 678,\n",
       " 'gold': 679,\n",
       " 'due': 680,\n",
       " 'fixed': 681,\n",
       " 'ended': 682,\n",
       " 'finally': 683,\n",
       " 'self': 684,\n",
       " 'force': 685,\n",
       " 'choice': 686,\n",
       " 'allowed': 687,\n",
       " 'late': 688,\n",
       " 'lots': 689,\n",
       " 'emergency': 690,\n",
       " 'tuition': 691,\n",
       " 'forth': 692,\n",
       " 'ing': 693,\n",
       " 'various': 694,\n",
       " 'war': 695,\n",
       " 'experience': 696,\n",
       " 'possible': 697,\n",
       " 'utilities': 698,\n",
       " 'aid': 699,\n",
       " 'foreign': 700,\n",
       " 'businesses': 701,\n",
       " 'county': 702,\n",
       " 'manage': 703,\n",
       " 'throw': 704,\n",
       " 'continue': 705,\n",
       " 'fine': 706,\n",
       " 'saved': 707,\n",
       " 'sitting': 708,\n",
       " 'quick': 709,\n",
       " 'tha': 710,\n",
       " 'offer': 711,\n",
       " 'happy': 712,\n",
       " 'depends': 713,\n",
       " 'ibm': 714,\n",
       " 'talked': 715,\n",
       " 'sound': 716,\n",
       " 'told': 717,\n",
       " 'opinion': 718,\n",
       " 'ask': 719,\n",
       " 'handy': 720,\n",
       " 'anywhere': 721,\n",
       " 'learned': 722,\n",
       " 'washington': 723,\n",
       " 'ow': 724,\n",
       " 'grocery': 725,\n",
       " 'oil': 726,\n",
       " 'individual': 727,\n",
       " 'district': 728,\n",
       " 'sears': 729,\n",
       " 'air': 730,\n",
       " 'become': 731,\n",
       " 'side': 732,\n",
       " 'goodness': 733,\n",
       " 'breaks': 734,\n",
       " 'knows': 735,\n",
       " 'fire': 736,\n",
       " 'plano': 737,\n",
       " 'bet': 738,\n",
       " 'mostly': 739,\n",
       " 'cause': 740,\n",
       " 'charges': 741,\n",
       " 'phone': 742,\n",
       " 'package': 743,\n",
       " 'gives': 744,\n",
       " 'hospital': 745,\n",
       " 'changed': 746,\n",
       " 'corporation': 747,\n",
       " 'corporations': 748,\n",
       " 'sharing': 749,\n",
       " 'charging': 750,\n",
       " 'issue': 751,\n",
       " 'unfortunately': 752,\n",
       " 'pennsylvania': 753,\n",
       " 'seemed': 754,\n",
       " 'law': 755,\n",
       " 'sudden': 756,\n",
       " 'wrong': 757,\n",
       " 'behind': 758,\n",
       " 'rich': 759,\n",
       " 'similar': 760,\n",
       " 'stores': 761,\n",
       " 'politicians': 762,\n",
       " 'excuse': 763,\n",
       " 'thirteen': 764,\n",
       " 'share': 765,\n",
       " 'available': 766,\n",
       " 'till': 767,\n",
       " 'father': 768,\n",
       " 'allow': 769,\n",
       " 'fast': 770,\n",
       " 'thank': 771,\n",
       " 'chance': 772,\n",
       " 'calls': 773,\n",
       " 'employee': 774,\n",
       " 'process': 775,\n",
       " 'master': 776,\n",
       " 'basis': 777,\n",
       " 'st': 778,\n",
       " 'numbers': 779,\n",
       " 'careful': 780,\n",
       " 'item': 781,\n",
       " 'compared': 782,\n",
       " 'thanks': 783,\n",
       " 'nobody': 784,\n",
       " 'provide': 785,\n",
       " 'investments': 786,\n",
       " 'somewhat': 787,\n",
       " 'list': 788,\n",
       " 'minutes': 789,\n",
       " 'housing': 790,\n",
       " 'huge': 791,\n",
       " 'telephone': 792,\n",
       " 'project': 793,\n",
       " 'enjoy': 794,\n",
       " 'gasoline': 795,\n",
       " 'strange': 796,\n",
       " 'checkbook': 797,\n",
       " 'automatically': 798,\n",
       " 'americans': 799,\n",
       " 'toward': 800,\n",
       " 'total': 801,\n",
       " 'helped': 802,\n",
       " 'ey': 803,\n",
       " 'finances': 804,\n",
       " 'writing': 805,\n",
       " 'teachers': 806,\n",
       " 'convenient': 807,\n",
       " 'smart': 808,\n",
       " 'eat': 809,\n",
       " 'retired': 810,\n",
       " 'son': 811,\n",
       " 'fees': 812,\n",
       " 'larger': 813,\n",
       " 'mess': 814,\n",
       " 'ye': 815,\n",
       " 'changing': 816,\n",
       " 'completely': 817,\n",
       " 'managed': 818,\n",
       " 'touch': 819,\n",
       " 'stock': 820,\n",
       " 'power': 821,\n",
       " 'trip': 822,\n",
       " 'seeing': 823,\n",
       " 'bush': 824,\n",
       " 'worst': 825,\n",
       " 'vote': 826,\n",
       " 'ell': 827,\n",
       " 'ooh': 828,\n",
       " 'love': 829,\n",
       " 'effect': 830,\n",
       " 'extent': 831,\n",
       " 'raised': 832,\n",
       " 'north': 833,\n",
       " 'hopefully': 834,\n",
       " 'wise': 835,\n",
       " 'built': 836,\n",
       " 'mistake': 837,\n",
       " 'obviously': 838,\n",
       " 'word': 839,\n",
       " 'nation': 840,\n",
       " 'fortunate': 841,\n",
       " 'trade': 842,\n",
       " 'tight': 843,\n",
       " 'amazing': 844,\n",
       " 'manager': 845,\n",
       " 'purchases': 846,\n",
       " 'play': 847,\n",
       " 'revenue': 848,\n",
       " 'europe': 849,\n",
       " 'space': 850,\n",
       " 'base': 851,\n",
       " 'particularly': 852,\n",
       " 'entire': 853,\n",
       " 'graduated': 854,\n",
       " 'society': 855,\n",
       " 'minute': 856,\n",
       " 'match': 857,\n",
       " 'estate': 858,\n",
       " 'consumer': 859,\n",
       " 'record': 860,\n",
       " 'hurt': 861,\n",
       " 'burden': 862,\n",
       " 'debts': 863,\n",
       " 'cheaper': 864,\n",
       " 'younger': 865,\n",
       " 'range': 866,\n",
       " 'apparently': 867,\n",
       " 'hospitals': 868,\n",
       " 'accept': 869,\n",
       " 'co': 870,\n",
       " 'books': 871,\n",
       " 'houses': 872,\n",
       " 'fill': 873,\n",
       " 'fall': 874,\n",
       " 'turned': 875,\n",
       " 'invest': 876,\n",
       " 'ira': 877,\n",
       " 'kay': 878,\n",
       " 'quarter': 879,\n",
       " 'main': 880,\n",
       " 'sat': 881,\n",
       " 'gee': 882,\n",
       " 'eleven': 883,\n",
       " 'answer': 884,\n",
       " 'across': 885,\n",
       " 'straight': 886,\n",
       " 'penny': 887,\n",
       " 'neat': 888,\n",
       " 'stopped': 889,\n",
       " 'caught': 890,\n",
       " 'owe': 891,\n",
       " 'party': 892,\n",
       " 'ideas': 893,\n",
       " 'afraid': 894,\n",
       " 'necessary': 895,\n",
       " 'none': 896,\n",
       " 'luck': 897,\n",
       " 'shopping': 898,\n",
       " 'familiar': 899,\n",
       " 'noticed': 900,\n",
       " 'calling': 901,\n",
       " 'losing': 902,\n",
       " 'sixteen': 903,\n",
       " 'ch': 904,\n",
       " 'changes': 905,\n",
       " 'management': 906,\n",
       " 'responsibility': 907,\n",
       " 'open': 908,\n",
       " 'wrote': 909,\n",
       " 'automatic': 910,\n",
       " 'friend': 911,\n",
       " 'tv': 912,\n",
       " 'create': 913,\n",
       " 'ere': 914,\n",
       " 'originally': 915,\n",
       " 'notice': 916,\n",
       " 'groups': 917,\n",
       " 'meet': 918,\n",
       " 'thousands': 919,\n",
       " 'form': 920,\n",
       " 'adds': 921,\n",
       " 'georgia': 922,\n",
       " 'board': 923,\n",
       " 'san': 924,\n",
       " 'corporate': 925,\n",
       " 'hi': 926,\n",
       " 'helping': 927,\n",
       " 'catch': 928,\n",
       " '401k': 929,\n",
       " 'file': 930,\n",
       " 'budgets': 931,\n",
       " 'items': 932,\n",
       " 'miss': 933,\n",
       " 'weekend': 934,\n",
       " 'pocket': 935,\n",
       " 'however': 936,\n",
       " 'massachusetts': 937,\n",
       " 'somehow': 938,\n",
       " 'room': 939,\n",
       " 'perhaps': 940,\n",
       " 'train': 941,\n",
       " 'quality': 942,\n",
       " 'knowledge': 943,\n",
       " 'rough': 944,\n",
       " 'beyond': 945,\n",
       " 'emergencies': 946,\n",
       " 'envelope': 947,\n",
       " 'reasons': 948,\n",
       " 'view': 949,\n",
       " 'projects': 950,\n",
       " 'directly': 951,\n",
       " 'engineering': 952,\n",
       " 'miles': 953,\n",
       " 'wha': 954,\n",
       " 'irs': 955,\n",
       " 'holidays': 956,\n",
       " 'earn': 957,\n",
       " 'firm': 958,\n",
       " 'stand': 959,\n",
       " 'written': 960,\n",
       " 'common': 961,\n",
       " 'fourteen': 962,\n",
       " 'moment': 963,\n",
       " 'deduct': 964,\n",
       " 'boss': 965,\n",
       " 'size': 966,\n",
       " 'anyone': 967,\n",
       " 'news': 968,\n",
       " 'bonds': 969,\n",
       " 'aetna': 970,\n",
       " 'expense': 971,\n",
       " 'dropped': 972,\n",
       " 'united': 973,\n",
       " 'bureaucracy': 974,\n",
       " 'separate': 975,\n",
       " 'pull': 976,\n",
       " 'capital': 977,\n",
       " 'likes': 978,\n",
       " 'flow': 979,\n",
       " 'deposit': 980,\n",
       " 'data': 981,\n",
       " 'serious': 982,\n",
       " 'runs': 983,\n",
       " 'overall': 984,\n",
       " 'water': 985,\n",
       " 'unfair': 986,\n",
       " 'wa': 987,\n",
       " 'hardly': 988,\n",
       " 'computers': 989,\n",
       " 'florida': 990,\n",
       " 'mentioned': 991,\n",
       " 'smaller': 992,\n",
       " 'moving': 993,\n",
       " 'teach': 994,\n",
       " 'engineer': 995,\n",
       " 'election': 996,\n",
       " 'hell': 997,\n",
       " 'became': 998,\n",
       " 'report': 999,\n",
       " 'cd': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "characteristic-lambda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenizer.word_index) + 1: 8216\n"
     ]
    }
   ],
   "source": [
    "file = open(\"tokenizer.pickle\",'rb')\n",
    "tokenizer1 = pickle.load(file)\n",
    "print(\"len(tokenizer.word_index) + 1:\",len(tokenizer1.word_index) + 1)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "processed-visit",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent': 1,\n",
       " 'customer': 2,\n",
       " 'silence': 3,\n",
       " 'uh': 4,\n",
       " 'know': 5,\n",
       " 'yeah': 6,\n",
       " 'um': 7,\n",
       " 'laughter': 8,\n",
       " 'noise': 9,\n",
       " 'well': 10,\n",
       " 'like': 11,\n",
       " 'right': 12,\n",
       " 'think': 13,\n",
       " 'hum': 14,\n",
       " 'huh': 15,\n",
       " 'get': 16,\n",
       " 'oh': 17,\n",
       " 'really': 18,\n",
       " 'one': 19,\n",
       " 'money': 20,\n",
       " 'pay': 21,\n",
       " 'got': 22,\n",
       " 'people': 23,\n",
       " 'going': 24,\n",
       " 'lot': 25,\n",
       " 'mean': 26,\n",
       " 'much': 27,\n",
       " 'things': 28,\n",
       " 'would': 29,\n",
       " 'good': 30,\n",
       " 'go': 31,\n",
       " 'something': 32,\n",
       " 'kind': 33,\n",
       " 'credit': 34,\n",
       " '1': 35,\n",
       " 'time': 36,\n",
       " 'work': 37,\n",
       " 'okay': 38,\n",
       " 'thing': 39,\n",
       " 'guess': 40,\n",
       " 'tax': 41,\n",
       " 'see': 42,\n",
       " 'vocalized': 43,\n",
       " 'way': 44,\n",
       " 'taxes': 45,\n",
       " 'dollars': 46,\n",
       " 'use': 47,\n",
       " 'year': 48,\n",
       " 'say': 49,\n",
       " 'years': 50,\n",
       " 'two': 51,\n",
       " 'budget': 52,\n",
       " 'little': 53,\n",
       " 'card': 54,\n",
       " 'month': 55,\n",
       " 'yes': 56,\n",
       " 'back': 57,\n",
       " 'pretty': 58,\n",
       " 'even': 59,\n",
       " 'take': 60,\n",
       " 'make': 61,\n",
       " 'every': 62,\n",
       " 'could': 63,\n",
       " 'us': 64,\n",
       " 'need': 65,\n",
       " 'benefits': 66,\n",
       " 'sure': 67,\n",
       " 'want': 68,\n",
       " 'put': 69,\n",
       " 'five': 70,\n",
       " 'company': 71,\n",
       " 'probably': 72,\n",
       " 'paying': 73,\n",
       " 'anything': 74,\n",
       " 'cards': 75,\n",
       " 'percent': 76,\n",
       " 'hundred': 77,\n",
       " 'never': 78,\n",
       " 'said': 79,\n",
       " 'always': 80,\n",
       " 'insurance': 81,\n",
       " 'keep': 82,\n",
       " 'real': 83,\n",
       " 'state': 84,\n",
       " 'government': 85,\n",
       " 'still': 86,\n",
       " 'getting': 87,\n",
       " 'long': 88,\n",
       " 'paid': 89,\n",
       " 'three': 90,\n",
       " 'spend': 91,\n",
       " 'actually': 92,\n",
       " 'big': 93,\n",
       " 'income': 94,\n",
       " 'come': 95,\n",
       " 'stuff': 96,\n",
       " 'try': 97,\n",
       " 'many': 98,\n",
       " 'everything': 99,\n",
       " 'job': 100,\n",
       " 'used': 101,\n",
       " 'interest': 102,\n",
       " 'sort': 103,\n",
       " 'maybe': 104,\n",
       " 'school': 105,\n",
       " 'also': 106,\n",
       " 'feel': 107,\n",
       " 'went': 108,\n",
       " 'bit': 109,\n",
       " 'buy': 110,\n",
       " 'seems': 111,\n",
       " 'enough': 112,\n",
       " 'day': 113,\n",
       " 'course': 114,\n",
       " 'look': 115,\n",
       " 'na': 116,\n",
       " 'nice': 117,\n",
       " 'twenty': 118,\n",
       " 'amount': 119,\n",
       " 'let': 120,\n",
       " 'better': 121,\n",
       " 'far': 122,\n",
       " 'true': 123,\n",
       " 'savings': 124,\n",
       " 'first': 125,\n",
       " 'give': 126,\n",
       " 'care': 127,\n",
       " 'able': 128,\n",
       " 'last': 129,\n",
       " 'trying': 130,\n",
       " 'around': 131,\n",
       " 'car': 132,\n",
       " 'house': 133,\n",
       " 'another': 134,\n",
       " 'whatever': 135,\n",
       " 'problem': 136,\n",
       " 'thousand': 137,\n",
       " 'end': 138,\n",
       " 'though': 139,\n",
       " 'husband': 140,\n",
       " 'cash': 141,\n",
       " 'part': 142,\n",
       " 'health': 143,\n",
       " 'check': 144,\n",
       " 'else': 145,\n",
       " 'four': 146,\n",
       " 'talking': 147,\n",
       " 'ti': 148,\n",
       " 'exactly': 149,\n",
       " 'bad': 150,\n",
       " 'hard': 151,\n",
       " 'great': 152,\n",
       " 'new': 153,\n",
       " 'whole': 154,\n",
       " 'cut': 155,\n",
       " 'start': 156,\n",
       " 'gon': 157,\n",
       " 'working': 158,\n",
       " 'away': 159,\n",
       " 'ten': 160,\n",
       " 'week': 161,\n",
       " 'goes': 162,\n",
       " 'different': 163,\n",
       " 'home': 164,\n",
       " 'account': 165,\n",
       " 'plan': 166,\n",
       " 'find': 167,\n",
       " 'family': 168,\n",
       " 'college': 169,\n",
       " 'sometimes': 170,\n",
       " 'charge': 171,\n",
       " 'kids': 172,\n",
       " 'least': 173,\n",
       " 'quite': 174,\n",
       " 'bills': 175,\n",
       " 'important': 176,\n",
       " 'either': 177,\n",
       " 'live': 178,\n",
       " 'done': 179,\n",
       " 'high': 180,\n",
       " 'spending': 181,\n",
       " 'business': 182,\n",
       " 'thought': 183,\n",
       " 'months': 184,\n",
       " 'texas': 185,\n",
       " 'anyway': 186,\n",
       " 'ever': 187,\n",
       " 'dollar': 188,\n",
       " 'bye': 189,\n",
       " 'fifty': 190,\n",
       " 'call': 191,\n",
       " 'point': 192,\n",
       " 'federal': 193,\n",
       " 'made': 194,\n",
       " 'works': 195,\n",
       " 'times': 196,\n",
       " 'worked': 197,\n",
       " 'debt': 198,\n",
       " 'bill': 199,\n",
       " 'tell': 200,\n",
       " 'might': 201,\n",
       " 'couple': 202,\n",
       " 'ago': 203,\n",
       " 'everybody': 204,\n",
       " 'saying': 205,\n",
       " 'six': 206,\n",
       " 'days': 207,\n",
       " 'th': 208,\n",
       " 'thirty': 209,\n",
       " 'vacation': 210,\n",
       " 'somebody': 211,\n",
       " 'may': 212,\n",
       " 'companies': 213,\n",
       " 'since': 214,\n",
       " 'salary': 215,\n",
       " 'comes': 216,\n",
       " 'less': 217,\n",
       " 'especially': 218,\n",
       " 'idea': 219,\n",
       " 'interesting': 220,\n",
       " 'gets': 221,\n",
       " 'agree': 222,\n",
       " 'fact': 223,\n",
       " 'wife': 224,\n",
       " 'started': 225,\n",
       " 'wow': 226,\n",
       " 'medical': 227,\n",
       " 'system': 228,\n",
       " 'certain': 229,\n",
       " 'children': 230,\n",
       " 'basically': 231,\n",
       " 'talk': 232,\n",
       " 'large': 233,\n",
       " 'set': 234,\n",
       " 'sales': 235,\n",
       " 'almost': 236,\n",
       " 'cost': 237,\n",
       " 'area': 238,\n",
       " 'term': 239,\n",
       " 'help': 240,\n",
       " 'life': 241,\n",
       " 'spent': 242,\n",
       " 'place': 243,\n",
       " 'weeks': 244,\n",
       " 'save': 245,\n",
       " 'w': 246,\n",
       " 'ones': 247,\n",
       " 'country': 248,\n",
       " 'usually': 249,\n",
       " 'eight': 250,\n",
       " 'american': 251,\n",
       " 'seem': 252,\n",
       " 'visa': 253,\n",
       " 'type': 254,\n",
       " 'expenses': 255,\n",
       " 'half': 256,\n",
       " 'bank': 257,\n",
       " 'making': 258,\n",
       " 'married': 259,\n",
       " 'small': 260,\n",
       " 'situation': 261,\n",
       " 'benefit': 262,\n",
       " 'whether': 263,\n",
       " 'ou': 264,\n",
       " 'ahead': 265,\n",
       " 'easy': 266,\n",
       " 'next': 267,\n",
       " 'write': 268,\n",
       " 'makes': 269,\n",
       " 'left': 270,\n",
       " 'extra': 271,\n",
       " 'old': 272,\n",
       " 'e': 273,\n",
       " 'retirement': 274,\n",
       " 'worth': 275,\n",
       " 'happen': 276,\n",
       " 'gone': 277,\n",
       " 'absolutely': 278,\n",
       " 'yet': 279,\n",
       " 'heard': 280,\n",
       " 'number': 281,\n",
       " 'dental': 282,\n",
       " 'believe': 283,\n",
       " 'yep': 284,\n",
       " 'loan': 285,\n",
       " 'without': 286,\n",
       " 'took': 287,\n",
       " 'understand': 288,\n",
       " 'rather': 289,\n",
       " 'mine': 290,\n",
       " 'hm': 291,\n",
       " 'found': 292,\n",
       " 'seven': 293,\n",
       " 'person': 294,\n",
       " 'fifteen': 295,\n",
       " 'major': 296,\n",
       " 'expensive': 297,\n",
       " 'run': 298,\n",
       " 'looking': 299,\n",
       " 'computer': 300,\n",
       " 'gotten': 301,\n",
       " 'monthly': 302,\n",
       " 'sounds': 303,\n",
       " 'came': 304,\n",
       " 'coming': 305,\n",
       " 'programs': 306,\n",
       " 'problems': 307,\n",
       " 'bought': 308,\n",
       " 'gas': 309,\n",
       " 'sense': 310,\n",
       " 'world': 311,\n",
       " 'aside': 312,\n",
       " 'already': 313,\n",
       " 'best': 314,\n",
       " 'taking': 315,\n",
       " 'control': 316,\n",
       " 'payment': 317,\n",
       " 'wanted': 318,\n",
       " 'living': 319,\n",
       " 'loans': 320,\n",
       " 'security': 321,\n",
       " 'rate': 322,\n",
       " 'higher': 323,\n",
       " 'full': 324,\n",
       " 'property': 325,\n",
       " 'checks': 326,\n",
       " 'someone': 327,\n",
       " 'n': 328,\n",
       " 'mind': 329,\n",
       " 'christmas': 330,\n",
       " 'figure': 331,\n",
       " 'jobs': 332,\n",
       " 'places': 333,\n",
       " 'change': 334,\n",
       " 'express': 335,\n",
       " 'within': 336,\n",
       " 'remember': 337,\n",
       " 'trouble': 338,\n",
       " 'union': 339,\n",
       " 'balance': 340,\n",
       " 'deal': 341,\n",
       " 'economy': 342,\n",
       " 'city': 343,\n",
       " 'costs': 344,\n",
       " 'per': 345,\n",
       " 'stay': 346,\n",
       " 'raise': 347,\n",
       " 'personal': 348,\n",
       " 'instead': 349,\n",
       " 'reason': 350,\n",
       " 'takes': 351,\n",
       " 'definitely': 352,\n",
       " 'dallas': 353,\n",
       " 'doctor': 354,\n",
       " 'today': 355,\n",
       " 'education': 356,\n",
       " 'program': 357,\n",
       " 'hand': 358,\n",
       " 'matter': 359,\n",
       " 'using': 360,\n",
       " 'free': 361,\n",
       " 'although': 362,\n",
       " 'case': 363,\n",
       " 'forty': 364,\n",
       " 'line': 365,\n",
       " 'fee': 366,\n",
       " 'supposed': 367,\n",
       " 'anymore': 368,\n",
       " 'budgeting': 369,\n",
       " 'called': 370,\n",
       " 'ah': 371,\n",
       " 'buying': 372,\n",
       " 'certainly': 373,\n",
       " 'eighty': 374,\n",
       " 'hear': 375,\n",
       " 'send': 376,\n",
       " 'terms': 377,\n",
       " 'financial': 378,\n",
       " 'thinking': 379,\n",
       " 'schools': 380,\n",
       " 'student': 381,\n",
       " 'nine': 382,\n",
       " 'fund': 383,\n",
       " 'social': 384,\n",
       " 'unless': 385,\n",
       " 'realize': 386,\n",
       " 'plus': 387,\n",
       " 'afford': 388,\n",
       " 'services': 389,\n",
       " 'national': 390,\n",
       " 'difference': 391,\n",
       " 'eighteen': 392,\n",
       " 'nothing': 393,\n",
       " 'seen': 394,\n",
       " 'food': 395,\n",
       " 'payments': 396,\n",
       " 'wish': 397,\n",
       " 'short': 398,\n",
       " 'guy': 399,\n",
       " 'countries': 400,\n",
       " 'single': 401,\n",
       " 'defense': 402,\n",
       " 'paper': 403,\n",
       " 'fairly': 404,\n",
       " 'topic': 405,\n",
       " 'fair': 406,\n",
       " 'groceries': 407,\n",
       " 'rates': 408,\n",
       " 'close': 409,\n",
       " 'cover': 410,\n",
       " 'wh': 411,\n",
       " 'saving': 412,\n",
       " 'congress': 413,\n",
       " 'hey': 414,\n",
       " 'anybody': 415,\n",
       " 'leave': 416,\n",
       " 'office': 417,\n",
       " 'limit': 418,\n",
       " 'discover': 419,\n",
       " 'somewhere': 420,\n",
       " 'deficit': 421,\n",
       " 'carry': 422,\n",
       " 'needs': 423,\n",
       " 'covered': 424,\n",
       " 'happens': 425,\n",
       " 'often': 426,\n",
       " 'boy': 427,\n",
       " 'ought': 428,\n",
       " 'happened': 429,\n",
       " 'level': 430,\n",
       " 'several': 431,\n",
       " 'rid': 432,\n",
       " 'local': 433,\n",
       " 'moved': 434,\n",
       " 'paycheck': 435,\n",
       " 'pays': 436,\n",
       " 'starting': 437,\n",
       " 'hope': 438,\n",
       " 'putting': 439,\n",
       " 'low': 440,\n",
       " 'percentage': 441,\n",
       " 'checking': 442,\n",
       " 'taken': 443,\n",
       " 'mortgage': 444,\n",
       " 'sit': 445,\n",
       " 'sixty': 446,\n",
       " 'tried': 447,\n",
       " 'ha': 448,\n",
       " 'plans': 449,\n",
       " 'banks': 450,\n",
       " 'taxed': 451,\n",
       " 'twelve': 452,\n",
       " 'running': 453,\n",
       " 'general': 454,\n",
       " 'concerned': 455,\n",
       " 'biggest': 456,\n",
       " 'turn': 457,\n",
       " 'recently': 458,\n",
       " 'lower': 459,\n",
       " 'c': 460,\n",
       " 'means': 461,\n",
       " 'second': 462,\n",
       " 'tough': 463,\n",
       " 'states': 464,\n",
       " 'period': 465,\n",
       " 'kinds': 466,\n",
       " 'h': 467,\n",
       " 'r': 468,\n",
       " 'town': 469,\n",
       " 'seventy': 470,\n",
       " 'based': 471,\n",
       " 'add': 472,\n",
       " 'gosh': 473,\n",
       " 'ninety': 474,\n",
       " 'middle': 475,\n",
       " 'ways': 476,\n",
       " 'must': 477,\n",
       " 'wonderful': 478,\n",
       " 'god': 479,\n",
       " 'past': 480,\n",
       " 'rest': 481,\n",
       " 'price': 482,\n",
       " 'gave': 483,\n",
       " 'top': 484,\n",
       " 'keeping': 485,\n",
       " 'military': 486,\n",
       " 'university': 487,\n",
       " 'sell': 488,\n",
       " 'knew': 489,\n",
       " 'order': 490,\n",
       " 'track': 491,\n",
       " 'waste': 492,\n",
       " 'pick': 493,\n",
       " 'wan': 494,\n",
       " 'age': 495,\n",
       " 'together': 496,\n",
       " 'hours': 497,\n",
       " 'daughter': 498,\n",
       " 'question': 499,\n",
       " 'given': 500,\n",
       " 'department': 501,\n",
       " 'f': 502,\n",
       " 'totally': 503,\n",
       " 'says': 504,\n",
       " 'lose': 505,\n",
       " 'service': 506,\n",
       " 'hmo': 507,\n",
       " 'market': 508,\n",
       " 'b': 509,\n",
       " 'degree': 510,\n",
       " 'bring': 511,\n",
       " 'nd': 512,\n",
       " 'public': 513,\n",
       " 'lived': 514,\n",
       " 'longer': 515,\n",
       " 'coverage': 516,\n",
       " 'cars': 517,\n",
       " 'employees': 518,\n",
       " 'organization': 519,\n",
       " 'bucks': 520,\n",
       " 'p': 521,\n",
       " 'everyone': 522,\n",
       " 'cutting': 523,\n",
       " 'wonder': 524,\n",
       " 'sick': 525,\n",
       " 'child': 526,\n",
       " 'policy': 527,\n",
       " 'hit': 528,\n",
       " 'consider': 529,\n",
       " 'sorry': 530,\n",
       " 'particular': 531,\n",
       " 'sale': 532,\n",
       " 'man': 533,\n",
       " 'president': 534,\n",
       " 'helps': 535,\n",
       " 'kept': 536,\n",
       " 'later': 537,\n",
       " 'nineteen': 538,\n",
       " 'felt': 539,\n",
       " 'guys': 540,\n",
       " 'funny': 541,\n",
       " 'wait': 542,\n",
       " 'minimum': 543,\n",
       " 'sent': 544,\n",
       " 'store': 545,\n",
       " 'mail': 546,\n",
       " 'mastercard': 547,\n",
       " 'awful': 548,\n",
       " 'welfare': 549,\n",
       " 'stick': 550,\n",
       " 'wants': 551,\n",
       " 'easier': 552,\n",
       " 'california': 553,\n",
       " 'build': 554,\n",
       " 'whenever': 555,\n",
       " 'lost': 556,\n",
       " 'twice': 557,\n",
       " 'instance': 558,\n",
       " 'systems': 559,\n",
       " 'purchase': 560,\n",
       " 'involved': 561,\n",
       " 'handle': 562,\n",
       " 'basic': 563,\n",
       " 'besides': 564,\n",
       " 'doctors': 565,\n",
       " 'giving': 566,\n",
       " 'older': 567,\n",
       " 'profit': 568,\n",
       " 'pension': 569,\n",
       " 'research': 570,\n",
       " 'york': 571,\n",
       " 'generally': 572,\n",
       " 'industry': 573,\n",
       " 'worry': 574,\n",
       " 'name': 575,\n",
       " 'charged': 576,\n",
       " 'value': 577,\n",
       " 'dad': 578,\n",
       " 'advantage': 579,\n",
       " 'private': 580,\n",
       " 'forget': 581,\n",
       " 'except': 582,\n",
       " 'mother': 583,\n",
       " 'move': 584,\n",
       " 'clothes': 585,\n",
       " 'third': 586,\n",
       " 'se': 587,\n",
       " 'l': 588,\n",
       " 'retire': 589,\n",
       " 'crazy': 590,\n",
       " 'u': 591,\n",
       " 'travel': 592,\n",
       " 'x': 593,\n",
       " 'funds': 594,\n",
       " 'cents': 595,\n",
       " 'worse': 596,\n",
       " 'tend': 597,\n",
       " 'million': 598,\n",
       " 'special': 599,\n",
       " 'night': 600,\n",
       " 'hate': 601,\n",
       " 'regular': 602,\n",
       " 'feeling': 603,\n",
       " 'bigger': 604,\n",
       " 'soon': 605,\n",
       " 'young': 606,\n",
       " 'billion': 607,\n",
       " 'break': 608,\n",
       " 'subject': 609,\n",
       " 'annual': 610,\n",
       " 'imagine': 611,\n",
       " 'opportunity': 612,\n",
       " 'read': 613,\n",
       " 'class': 614,\n",
       " 'ready': 615,\n",
       " 'looked': 616,\n",
       " 'fun': 617,\n",
       " 'friends': 618,\n",
       " 'position': 619,\n",
       " 'outside': 620,\n",
       " 'willing': 621,\n",
       " 'suppose': 622,\n",
       " 'along': 623,\n",
       " 'support': 624,\n",
       " 'interested': 625,\n",
       " 'future': 626,\n",
       " 'average': 627,\n",
       " 'eventually': 628,\n",
       " 'rent': 629,\n",
       " 'needed': 630,\n",
       " 'return': 631,\n",
       " 'investment': 632,\n",
       " 'poor': 633,\n",
       " 'increase': 634,\n",
       " 'early': 635,\n",
       " 'hold': 636,\n",
       " 'planning': 637,\n",
       " 'terrible': 638,\n",
       " 'homes': 639,\n",
       " 'stop': 640,\n",
       " 'example': 641,\n",
       " 'keeps': 642,\n",
       " 'finance': 643,\n",
       " 'learn': 644,\n",
       " 'building': 645,\n",
       " 'decided': 646,\n",
       " 'en': 647,\n",
       " 'graduate': 648,\n",
       " 'show': 649,\n",
       " 'parents': 650,\n",
       " 'difficult': 651,\n",
       " 'deductible': 652,\n",
       " 'watch': 653,\n",
       " 'glad': 654,\n",
       " 'beginning': 655,\n",
       " 'saw': 656,\n",
       " 'areas': 657,\n",
       " 'necessarily': 658,\n",
       " 'summer': 659,\n",
       " 'lucky': 660,\n",
       " 'book': 661,\n",
       " 'accounts': 662,\n",
       " 'drive': 663,\n",
       " 'towards': 664,\n",
       " 'responsible': 665,\n",
       " 'bunch': 666,\n",
       " 'quit': 667,\n",
       " 'decide': 668,\n",
       " 'k': 669,\n",
       " 'deductions': 670,\n",
       " 'others': 671,\n",
       " 'hour': 672,\n",
       " 'scary': 673,\n",
       " 'prices': 674,\n",
       " 'ridiculous': 675,\n",
       " 'machine': 676,\n",
       " 'g': 677,\n",
       " 'group': 678,\n",
       " 'gold': 679,\n",
       " 'due': 680,\n",
       " 'fixed': 681,\n",
       " 'ended': 682,\n",
       " 'finally': 683,\n",
       " 'self': 684,\n",
       " 'force': 685,\n",
       " 'choice': 686,\n",
       " 'allowed': 687,\n",
       " 'late': 688,\n",
       " 'lots': 689,\n",
       " 'emergency': 690,\n",
       " 'tuition': 691,\n",
       " 'forth': 692,\n",
       " 'ing': 693,\n",
       " 'various': 694,\n",
       " 'war': 695,\n",
       " 'experience': 696,\n",
       " 'possible': 697,\n",
       " 'utilities': 698,\n",
       " 'aid': 699,\n",
       " 'foreign': 700,\n",
       " 'businesses': 701,\n",
       " 'county': 702,\n",
       " 'manage': 703,\n",
       " 'throw': 704,\n",
       " 'continue': 705,\n",
       " 'fine': 706,\n",
       " 'saved': 707,\n",
       " 'sitting': 708,\n",
       " 'quick': 709,\n",
       " 'tha': 710,\n",
       " 'offer': 711,\n",
       " 'happy': 712,\n",
       " 'depends': 713,\n",
       " 'ibm': 714,\n",
       " 'talked': 715,\n",
       " 'sound': 716,\n",
       " 'told': 717,\n",
       " 'opinion': 718,\n",
       " 'ask': 719,\n",
       " 'handy': 720,\n",
       " 'anywhere': 721,\n",
       " 'learned': 722,\n",
       " 'washington': 723,\n",
       " 'ow': 724,\n",
       " 'grocery': 725,\n",
       " 'oil': 726,\n",
       " 'individual': 727,\n",
       " 'district': 728,\n",
       " 'sears': 729,\n",
       " 'air': 730,\n",
       " 'become': 731,\n",
       " 'side': 732,\n",
       " 'goodness': 733,\n",
       " 'breaks': 734,\n",
       " 'knows': 735,\n",
       " 'fire': 736,\n",
       " 'plano': 737,\n",
       " 'bet': 738,\n",
       " 'mostly': 739,\n",
       " 'cause': 740,\n",
       " 'charges': 741,\n",
       " 'phone': 742,\n",
       " 'package': 743,\n",
       " 'gives': 744,\n",
       " 'hospital': 745,\n",
       " 'changed': 746,\n",
       " 'corporation': 747,\n",
       " 'corporations': 748,\n",
       " 'sharing': 749,\n",
       " 'charging': 750,\n",
       " 'issue': 751,\n",
       " 'unfortunately': 752,\n",
       " 'pennsylvania': 753,\n",
       " 'seemed': 754,\n",
       " 'law': 755,\n",
       " 'sudden': 756,\n",
       " 'wrong': 757,\n",
       " 'behind': 758,\n",
       " 'rich': 759,\n",
       " 'similar': 760,\n",
       " 'stores': 761,\n",
       " 'politicians': 762,\n",
       " 'excuse': 763,\n",
       " 'thirteen': 764,\n",
       " 'share': 765,\n",
       " 'available': 766,\n",
       " 'till': 767,\n",
       " 'father': 768,\n",
       " 'allow': 769,\n",
       " 'fast': 770,\n",
       " 'thank': 771,\n",
       " 'chance': 772,\n",
       " 'calls': 773,\n",
       " 'employee': 774,\n",
       " 'process': 775,\n",
       " 'master': 776,\n",
       " 'basis': 777,\n",
       " 'st': 778,\n",
       " 'numbers': 779,\n",
       " 'careful': 780,\n",
       " 'item': 781,\n",
       " 'compared': 782,\n",
       " 'thanks': 783,\n",
       " 'nobody': 784,\n",
       " 'provide': 785,\n",
       " 'investments': 786,\n",
       " 'somewhat': 787,\n",
       " 'list': 788,\n",
       " 'minutes': 789,\n",
       " 'housing': 790,\n",
       " 'huge': 791,\n",
       " 'telephone': 792,\n",
       " 'project': 793,\n",
       " 'enjoy': 794,\n",
       " 'gasoline': 795,\n",
       " 'strange': 796,\n",
       " 'checkbook': 797,\n",
       " 'automatically': 798,\n",
       " 'americans': 799,\n",
       " 'toward': 800,\n",
       " 'total': 801,\n",
       " 'helped': 802,\n",
       " 'ey': 803,\n",
       " 'finances': 804,\n",
       " 'writing': 805,\n",
       " 'teachers': 806,\n",
       " 'convenient': 807,\n",
       " 'smart': 808,\n",
       " 'eat': 809,\n",
       " 'retired': 810,\n",
       " 'son': 811,\n",
       " 'fees': 812,\n",
       " 'larger': 813,\n",
       " 'mess': 814,\n",
       " 'ye': 815,\n",
       " 'changing': 816,\n",
       " 'completely': 817,\n",
       " 'managed': 818,\n",
       " 'touch': 819,\n",
       " 'stock': 820,\n",
       " 'power': 821,\n",
       " 'trip': 822,\n",
       " 'seeing': 823,\n",
       " 'bush': 824,\n",
       " 'worst': 825,\n",
       " 'vote': 826,\n",
       " 'ell': 827,\n",
       " 'ooh': 828,\n",
       " 'love': 829,\n",
       " 'effect': 830,\n",
       " 'extent': 831,\n",
       " 'raised': 832,\n",
       " 'north': 833,\n",
       " 'hopefully': 834,\n",
       " 'wise': 835,\n",
       " 'built': 836,\n",
       " 'mistake': 837,\n",
       " 'obviously': 838,\n",
       " 'word': 839,\n",
       " 'nation': 840,\n",
       " 'fortunate': 841,\n",
       " 'trade': 842,\n",
       " 'tight': 843,\n",
       " 'amazing': 844,\n",
       " 'manager': 845,\n",
       " 'purchases': 846,\n",
       " 'play': 847,\n",
       " 'revenue': 848,\n",
       " 'europe': 849,\n",
       " 'space': 850,\n",
       " 'base': 851,\n",
       " 'particularly': 852,\n",
       " 'entire': 853,\n",
       " 'graduated': 854,\n",
       " 'society': 855,\n",
       " 'minute': 856,\n",
       " 'match': 857,\n",
       " 'estate': 858,\n",
       " 'consumer': 859,\n",
       " 'record': 860,\n",
       " 'hurt': 861,\n",
       " 'burden': 862,\n",
       " 'debts': 863,\n",
       " 'cheaper': 864,\n",
       " 'younger': 865,\n",
       " 'range': 866,\n",
       " 'apparently': 867,\n",
       " 'hospitals': 868,\n",
       " 'accept': 869,\n",
       " 'co': 870,\n",
       " 'books': 871,\n",
       " 'houses': 872,\n",
       " 'fill': 873,\n",
       " 'fall': 874,\n",
       " 'turned': 875,\n",
       " 'invest': 876,\n",
       " 'ira': 877,\n",
       " 'kay': 878,\n",
       " 'quarter': 879,\n",
       " 'main': 880,\n",
       " 'sat': 881,\n",
       " 'gee': 882,\n",
       " 'eleven': 883,\n",
       " 'answer': 884,\n",
       " 'across': 885,\n",
       " 'straight': 886,\n",
       " 'penny': 887,\n",
       " 'neat': 888,\n",
       " 'stopped': 889,\n",
       " 'caught': 890,\n",
       " 'owe': 891,\n",
       " 'party': 892,\n",
       " 'ideas': 893,\n",
       " 'afraid': 894,\n",
       " 'necessary': 895,\n",
       " 'none': 896,\n",
       " 'luck': 897,\n",
       " 'shopping': 898,\n",
       " 'familiar': 899,\n",
       " 'noticed': 900,\n",
       " 'calling': 901,\n",
       " 'losing': 902,\n",
       " 'sixteen': 903,\n",
       " 'ch': 904,\n",
       " 'changes': 905,\n",
       " 'management': 906,\n",
       " 'responsibility': 907,\n",
       " 'open': 908,\n",
       " 'wrote': 909,\n",
       " 'automatic': 910,\n",
       " 'friend': 911,\n",
       " 'tv': 912,\n",
       " 'create': 913,\n",
       " 'ere': 914,\n",
       " 'originally': 915,\n",
       " 'notice': 916,\n",
       " 'groups': 917,\n",
       " 'meet': 918,\n",
       " 'thousands': 919,\n",
       " 'form': 920,\n",
       " 'adds': 921,\n",
       " 'georgia': 922,\n",
       " 'board': 923,\n",
       " 'san': 924,\n",
       " 'corporate': 925,\n",
       " 'hi': 926,\n",
       " 'helping': 927,\n",
       " 'catch': 928,\n",
       " '401k': 929,\n",
       " 'file': 930,\n",
       " 'budgets': 931,\n",
       " 'items': 932,\n",
       " 'miss': 933,\n",
       " 'weekend': 934,\n",
       " 'pocket': 935,\n",
       " 'however': 936,\n",
       " 'massachusetts': 937,\n",
       " 'somehow': 938,\n",
       " 'room': 939,\n",
       " 'perhaps': 940,\n",
       " 'train': 941,\n",
       " 'quality': 942,\n",
       " 'knowledge': 943,\n",
       " 'rough': 944,\n",
       " 'beyond': 945,\n",
       " 'emergencies': 946,\n",
       " 'envelope': 947,\n",
       " 'reasons': 948,\n",
       " 'view': 949,\n",
       " 'projects': 950,\n",
       " 'directly': 951,\n",
       " 'engineering': 952,\n",
       " 'miles': 953,\n",
       " 'wha': 954,\n",
       " 'irs': 955,\n",
       " 'holidays': 956,\n",
       " 'earn': 957,\n",
       " 'firm': 958,\n",
       " 'stand': 959,\n",
       " 'written': 960,\n",
       " 'common': 961,\n",
       " 'fourteen': 962,\n",
       " 'moment': 963,\n",
       " 'deduct': 964,\n",
       " 'boss': 965,\n",
       " 'size': 966,\n",
       " 'anyone': 967,\n",
       " 'news': 968,\n",
       " 'bonds': 969,\n",
       " 'aetna': 970,\n",
       " 'expense': 971,\n",
       " 'dropped': 972,\n",
       " 'united': 973,\n",
       " 'bureaucracy': 974,\n",
       " 'separate': 975,\n",
       " 'pull': 976,\n",
       " 'capital': 977,\n",
       " 'likes': 978,\n",
       " 'flow': 979,\n",
       " 'deposit': 980,\n",
       " 'data': 981,\n",
       " 'serious': 982,\n",
       " 'runs': 983,\n",
       " 'overall': 984,\n",
       " 'water': 985,\n",
       " 'unfair': 986,\n",
       " 'wa': 987,\n",
       " 'hardly': 988,\n",
       " 'computers': 989,\n",
       " 'florida': 990,\n",
       " 'mentioned': 991,\n",
       " 'smaller': 992,\n",
       " 'moving': 993,\n",
       " 'teach': 994,\n",
       " 'engineer': 995,\n",
       " 'election': 996,\n",
       " 'hell': 997,\n",
       " 'became': 998,\n",
       " 'report': 999,\n",
       " 'cd': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "prescription-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weight_matrix = getEmbeddingWeightMatrix(embedding_vectors, tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "present-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('embedding_weight_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(embedding_weight_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "previous-chance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "identical-vaccine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8216, 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weight_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "coated-diagnosis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_tag</th>\n",
       "      <th>file_tag_encoder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_tag  file_tag_encoder\n",
       "0   Credit Card                 2\n",
       "1   Credit Card                 2\n",
       "2   Credit Card                 2\n",
       "3   Credit Card                 2\n",
       "4   Credit Card                 2\n",
       "5   Credit Card                 2\n",
       "6   Credit Card                 2\n",
       "7   Credit Card                 2\n",
       "8   Credit Card                 2\n",
       "9   Credit Card                 2\n",
       "10  Credit Card                 2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Doing Label Encoding the target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "data[\"file_tag_encoder\"] = lb_make.fit_transform(data.file_tag.astype(str))\n",
    "data[[\"file_tag\", \"file_tag_encoder\"]].head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "finite-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing Stratify spit the as the data also imbalanced\n",
    "train_x, Test_x, train_y, Test_y = model_selection.train_test_split(train_data_need, data['file_tag_encoder'],test_size=0.20, random_state=42, stratify=data['file_tag_encoder'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "understanding-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into test and Validation\n",
    "val_x, test_x, val_y, test_y = model_selection.train_test_split(Test_x, Test_y,test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "manufactured-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y1_train1 = np_utils.to_categorical(train_y)\n",
    "y1_test = np_utils.to_categorical(test_y)\n",
    "y1_val=np_utils.to_categorical(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "renewable-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Attention Layer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "class AttentionWithContext(Layer):    \n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        \n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "educated-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_maxlen=1680\n",
    "word_embed_size = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "planned-tragedy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_train1.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cleared-proxy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 1680)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "liquid-warehouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\keras-tf\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\keras-tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3144: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "##Model Building on LSTM and  Attention layer\n",
    "input = Input(shape=(train_x.shape[1],))\n",
    "\n",
    "inner = Embedding(input_dim=vocab_size, output_dim=word_embed_size, \n",
    "                   input_length=seq_maxlen, weights=[embedding_weight_matrix], \n",
    "                   trainable = True) (input)\n",
    "\n",
    "inner = LSTM(100, return_sequences=True)(inner)\n",
    "inner = Dropout(0.3)(inner)\n",
    "# compute importance for each step\n",
    "#inner = TimeDistributed(Dense(100))(inner) \n",
    "words_att = AttentionWithContext()(inner)\n",
    "\n",
    "inner = Dense(100, activation='relu')(words_att)\n",
    "#inner = Dense(40, activation='relu')(inner)\n",
    "output = Dense(y1_train1.shape[1], activation='softmax')(inner)\n",
    "\n",
    "model_a = Model(inputs = input, outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ancient-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.save('my_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "behavioral-faculty",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\keras-tf\\lib\\site-packages\\keras\\models.py:282: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "model_a1 = keras.models.load_model('my_lstm_model.h5', custom_objects={'AttentionWithContext':AttentionWithContext()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "announced-pleasure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1680)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1680, 100)         821600    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1680, 100)         80400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1680, 100)         0         \n",
      "_________________________________________________________________\n",
      "attention_with_context_1 (At (None, 100)               10200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 922,906\n",
      "Trainable params: 922,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_a1.summary())\n",
    "model_a1.compile(Adam(lr=0.01), 'categorical_crossentropy', metrics=['accuracy'])\n",
    "#model_a.load_weights('/var/www/projects/nehru/text_classification/code/model_incidents_attention_lstm_word_vec_100d_n1.h5')\n",
    "\n",
    "save_weights = ModelCheckpoint('model_doc_tagging_attention_lstm_word_vec_100d_new.h5', monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "relevant-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "model_json = model_a1.to_json()\n",
    "with open(\"model_incident_classification_lstm_attention.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "present-domain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\keras-tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 192 samples, validate on 38 samples\n",
      "Epoch 1/30\n",
      "192/192 [==============================] - 9s 47ms/step - loss: 1.8574 - acc: 0.1302 - val_loss: 1.7234 - val_acc: 0.2895\n",
      "Epoch 2/30\n",
      "192/192 [==============================] - 8s 41ms/step - loss: 1.6162 - acc: 0.3073 - val_loss: 1.5576 - val_acc: 0.2632\n",
      "Epoch 3/30\n",
      "192/192 [==============================] - 8s 42ms/step - loss: 1.3082 - acc: 0.5312 - val_loss: 1.2284 - val_acc: 0.6316\n",
      "Epoch 4/30\n",
      "192/192 [==============================] - 8s 43ms/step - loss: 0.9096 - acc: 0.6823 - val_loss: 0.8917 - val_acc: 0.7632\n",
      "Epoch 5/30\n",
      "192/192 [==============================] - 8s 44ms/step - loss: 0.5690 - acc: 0.8333 - val_loss: 0.3783 - val_acc: 0.8947\n",
      "Epoch 6/30\n",
      "192/192 [==============================] - 8s 44ms/step - loss: 0.3118 - acc: 0.9167 - val_loss: 0.3332 - val_acc: 0.9211\n",
      "Epoch 7/30\n",
      "192/192 [==============================] - 8s 44ms/step - loss: 0.1982 - acc: 0.9219 - val_loss: 0.3379 - val_acc: 0.8684\n",
      "Epoch 8/30\n",
      "192/192 [==============================] - 8s 44ms/step - loss: 0.1104 - acc: 0.9531 - val_loss: 0.3245 - val_acc: 0.8684\n",
      "Epoch 9/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 0.0448 - acc: 0.9844 - val_loss: 0.5930 - val_acc: 0.8684\n",
      "Epoch 10/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 0.0161 - acc: 1.0000 - val_loss: 0.6754 - val_acc: 0.8684\n",
      "Epoch 11/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.5086 - val_acc: 0.8684\n",
      "Epoch 12/30\n",
      "192/192 [==============================] - 9s 46ms/step - loss: 0.0117 - acc: 0.9948 - val_loss: 0.5452 - val_acc: 0.8684\n",
      "Epoch 13/30\n",
      "192/192 [==============================] - 9s 46ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.9240 - val_acc: 0.8158\n",
      "Epoch 14/30\n",
      "192/192 [==============================] - 9s 46ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.3368 - val_acc: 0.8947\n",
      "Epoch 15/30\n",
      "192/192 [==============================] - 9s 46ms/step - loss: 0.0063 - acc: 0.9948 - val_loss: 0.2237 - val_acc: 0.9474\n",
      "Epoch 16/30\n",
      "192/192 [==============================] - 9s 46ms/step - loss: 0.0156 - acc: 0.9948 - val_loss: 0.3791 - val_acc: 0.8947\n",
      "Epoch 17/30\n",
      "192/192 [==============================] - 9s 46ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.4146 - val_acc: 0.9211\n",
      "Epoch 18/30\n",
      "192/192 [==============================] - 9s 46ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.3922 - val_acc: 0.8947\n",
      "Epoch 19/30\n",
      "192/192 [==============================] - 9s 47ms/step - loss: 2.5006e-04 - acc: 1.0000 - val_loss: 0.3886 - val_acc: 0.8947\n",
      "Epoch 20/30\n",
      "192/192 [==============================] - 9s 47ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.4164 - val_acc: 0.8421\n",
      "Epoch 21/30\n",
      "192/192 [==============================] - 9s 47ms/step - loss: 2.4847e-04 - acc: 1.0000 - val_loss: 0.5641 - val_acc: 0.8158\n",
      "Epoch 22/30\n",
      "192/192 [==============================] - 9s 47ms/step - loss: 2.7867e-04 - acc: 1.0000 - val_loss: 0.6639 - val_acc: 0.7895\n",
      "Epoch 23/30\n",
      "192/192 [==============================] - 9s 47ms/step - loss: 3.6079e-04 - acc: 1.0000 - val_loss: 0.6773 - val_acc: 0.8421\n",
      "Epoch 24/30\n",
      "192/192 [==============================] - 9s 47ms/step - loss: 3.6072e-04 - acc: 1.0000 - val_loss: 0.6230 - val_acc: 0.8421\n",
      "Epoch 25/30\n",
      "192/192 [==============================] - 9s 47ms/step - loss: 4.3791e-04 - acc: 1.0000 - val_loss: 0.5196 - val_acc: 0.8947\n",
      "Epoch 26/30\n",
      "192/192 [==============================] - 9s 47ms/step - loss: 1.3963e-04 - acc: 1.0000 - val_loss: 0.4785 - val_acc: 0.8947\n",
      "Epoch 27/30\n",
      "192/192 [==============================] - 9s 48ms/step - loss: 1.7444e-04 - acc: 1.0000 - val_loss: 0.4573 - val_acc: 0.8947\n",
      "Epoch 28/30\n",
      "192/192 [==============================] - 9s 49ms/step - loss: 1.7372e-04 - acc: 1.0000 - val_loss: 0.4444 - val_acc: 0.8947\n",
      "Epoch 29/30\n",
      "192/192 [==============================] - 10s 50ms/step - loss: 1.2964e-04 - acc: 1.0000 - val_loss: 0.4328 - val_acc: 0.9211\n",
      "Epoch 30/30\n",
      "192/192 [==============================] - 9s 49ms/step - loss: 1.4099e-04 - acc: 1.0000 - val_loss: 0.4255 - val_acc: 0.9474\n"
     ]
    }
   ],
   "source": [
    "history = model_a1.fit(x=train_x, y=y1_train1, batch_size=32,\n",
    "          epochs=30, validation_data=(val_x, y1_val), callbacks=[save_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-formula",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "signed-world",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predicting on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "careful-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lstm=model_a1.predict(test_x)\n",
    "predicted_lstm=predicted_lstm.argmax(axis=-1)\n",
    "y1_test=y1_test.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "wanted-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy1=np.mean(predicted_lstm==y1_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "saved-africa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-signature",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "massive-lawsuit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 4, 3, 5, 4, 3, 5, 4, 5], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "clean-queens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 4, 3, 5, 4, 3, 5, 4, 5], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-generator",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "particular-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Model Building on LSTM and  Attention layer\n",
    "input = Input(shape=(train_x.shape[1],))\n",
    "\n",
    "inner = Embedding(input_dim=vocab_size, output_dim=word_embed_size, \n",
    "                   input_length=seq_maxlen, weights=[embedding_weight_matrix], \n",
    "                   trainable = True) (input)\n",
    "\n",
    "inner = GRU(100, return_sequences=True)(inner)\n",
    "inner = Dropout(0.3)(inner)\n",
    "# compute importance for each step\n",
    "#inner = TimeDistributed(Dense(100))(inner) \n",
    "words_att = AttentionWithContext()(inner)\n",
    "\n",
    "inner = Dense(100, activation='relu')(words_att)\n",
    "#inner = Dense(40, activation='relu')(inner)\n",
    "output = Dense(y1_train1.shape[1], activation='softmax')(inner)\n",
    "\n",
    "model_a = Model(inputs = input, outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "religious-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.save('my_GRU_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "entertaining-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "model_a1 = keras.models.load_model('my_GRU_model.h5', custom_objects={'AttentionWithContext':AttentionWithContext()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "characteristic-lunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 1680)              0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 1680, 100)         821600    \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 1680, 100)         60300     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1680, 100)         0         \n",
      "_________________________________________________________________\n",
      "attention_with_context_9 (At (None, 100)               10200     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 902,806\n",
      "Trainable params: 902,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_a1.summary())\n",
    "model_a1.compile(Adam(lr=0.01), 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "worst-wilderness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1680)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 1680, 100)         821600    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 1680, 100)         60300     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1680, 100)         0         \n",
      "_________________________________________________________________\n",
      "attention_with_context_7 (At (None, 100)               10200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 902,806\n",
      "Trainable params: 902,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model_a.load_weights('/var/www/projects/nehru/text_classification/code/model_incidents_attention_lstm_word_vec_100d_n1.h5')\n",
    "\n",
    "save_weights = ModelCheckpoint('model_doc_tagging_attention_GRU_glove_vec_100d_new.h5', monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "trying-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "model_json = model_a1.to_json()\n",
    "with open(\"model_incident_classification_gru_attention.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-blogger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "painful-pixel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\keras-tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 192 samples, validate on 38 samples\n",
      "Epoch 1/30\n",
      "192/192 [==============================] - 9s 47ms/step - loss: 1.7419 - acc: 0.2656 - val_loss: 1.6465 - val_acc: 0.2105\n",
      "Epoch 2/30\n",
      "192/192 [==============================] - 8s 40ms/step - loss: 1.2736 - acc: 0.5417 - val_loss: 1.2453 - val_acc: 0.6316\n",
      "Epoch 3/30\n",
      "192/192 [==============================] - 8s 42ms/step - loss: 0.8104 - acc: 0.7188 - val_loss: 1.1854 - val_acc: 0.6579\n",
      "Epoch 4/30\n",
      "192/192 [==============================] - 8s 42ms/step - loss: 0.6942 - acc: 0.8281 - val_loss: 0.5399 - val_acc: 0.8158\n",
      "Epoch 5/30\n",
      "192/192 [==============================] - 9s 47ms/step - loss: 0.4071 - acc: 0.8750 - val_loss: 0.4024 - val_acc: 0.8947\n",
      "Epoch 6/30\n",
      "192/192 [==============================] - 8s 44ms/step - loss: 0.1705 - acc: 0.9479 - val_loss: 0.4987 - val_acc: 0.8158\n",
      "Epoch 7/30\n",
      "192/192 [==============================] - 8s 44ms/step - loss: 0.0714 - acc: 0.9844 - val_loss: 0.3849 - val_acc: 0.8684\n",
      "Epoch 8/30\n",
      "192/192 [==============================] - 9s 44ms/step - loss: 0.0488 - acc: 0.9896 - val_loss: 0.3340 - val_acc: 0.8421\n",
      "Epoch 9/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 0.0310 - acc: 0.9948 - val_loss: 0.3319 - val_acc: 0.9474\n",
      "Epoch 10/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 0.0204 - acc: 0.9948 - val_loss: 0.4538 - val_acc: 0.8421\n",
      "Epoch 11/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 0.0087 - acc: 0.9948 - val_loss: 0.6372 - val_acc: 0.8684\n",
      "Epoch 12/30\n",
      "192/192 [==============================] - 9s 44ms/step - loss: 0.0134 - acc: 0.9948 - val_loss: 0.7299 - val_acc: 0.8158\n",
      "Epoch 13/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 0.0093 - acc: 1.0000 - val_loss: 0.4577 - val_acc: 0.8684\n",
      "Epoch 14/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 0.0044 - acc: 0.9948 - val_loss: 0.0779 - val_acc: 0.9737\n",
      "Epoch 15/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.2466 - val_acc: 0.9211\n",
      "Epoch 16/30\n",
      "192/192 [==============================] - 9s 44ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.5409 - val_acc: 0.9211\n",
      "Epoch 17/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 1.2276 - val_acc: 0.8421\n",
      "Epoch 18/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 9.2239e-04 - acc: 1.0000 - val_loss: 1.7183 - val_acc: 0.8158\n",
      "Epoch 19/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 5.4541e-04 - acc: 1.0000 - val_loss: 1.8486 - val_acc: 0.8158\n",
      "Epoch 20/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 9.0316e-04 - acc: 1.0000 - val_loss: 1.7275 - val_acc: 0.8158\n",
      "Epoch 21/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 5.7599e-04 - acc: 1.0000 - val_loss: 1.6221 - val_acc: 0.8421\n",
      "Epoch 22/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 0.0153 - acc: 0.9948 - val_loss: 1.6304 - val_acc: 0.8684\n",
      "Epoch 23/30\n",
      "192/192 [==============================] - 9s 46ms/step - loss: 0.1654 - acc: 0.9583 - val_loss: 1.4044 - val_acc: 0.7105\n",
      "Epoch 24/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 0.0837 - acc: 0.9635 - val_loss: 0.5606 - val_acc: 0.8947\n",
      "Epoch 25/30\n",
      "192/192 [==============================] - 9s 46ms/step - loss: 0.1696 - acc: 0.9635 - val_loss: 1.4387 - val_acc: 0.7105\n",
      "Epoch 26/30\n",
      "192/192 [==============================] - 9s 46ms/step - loss: 0.0636 - acc: 0.9688 - val_loss: 0.5475 - val_acc: 0.8947\n",
      "Epoch 27/30\n",
      "192/192 [==============================] - 9s 46ms/step - loss: 0.0490 - acc: 0.9740 - val_loss: 0.5258 - val_acc: 0.8158\n",
      "Epoch 28/30\n",
      "192/192 [==============================] - 9s 46ms/step - loss: 0.1173 - acc: 0.9792 - val_loss: 0.6233 - val_acc: 0.8684\n",
      "Epoch 29/30\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 0.0162 - acc: 0.9948 - val_loss: 0.4891 - val_acc: 0.8947\n",
      "Epoch 30/30\n",
      "192/192 [==============================] - 9s 49ms/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.4621 - val_acc: 0.9211\n"
     ]
    }
   ],
   "source": [
    "history = model_a1.fit(x=train_x, y=y1_train1, batch_size=32,\n",
    "          epochs=30, validation_data=(val_x, y1_val), callbacks=[save_weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-sellers",
   "metadata": {},
   "source": [
    "# Predicting on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "premier-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lstm=model_a1.predict(test_x)\n",
    "predicted_lstm=predicted_lstm.argmax(axis=-1)\n",
    "y1_test=y1_test.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "naked-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy1=np.mean(predicted_lstm==y1_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "frank-orleans",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 4, 3, 5, 4, 3, 5, 4, 5], dtype=int64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "written-inflation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "polar-world",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 4, 3, 5, 4, 3, 5, 4, 5], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "vocal-economics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 4, 3, 5, 4, 3, 5, 4, 5], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-return",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-slope",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-junior",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "offensive-uniform",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenizer.word_index) + 1: 8216\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    agent silence customer right agent okay custom...\n",
      "Name: clean_file_content, dtype: object\n",
      "(1, 1680)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Mar/2021 04:35:34] \"\u001b[37mPOST /getfile HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.5349399e-05 7.3971641e-08 9.9985337e-01 9.1207308e-05 6.5305089e-06\n",
      "  3.4526777e-06]]\n",
      "Credit Card\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Mar/2021 04:36:05] \"\u001b[37mPOST /getfile HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    agent silence customer noise customer okay um ...\n",
      "Name: clean_file_content, dtype: object\n",
      "(1, 1680)\n",
      "[[2.1722869e-06 4.9526159e-02 5.7222835e-05 9.5030171e-01 1.0769077e-04\n",
      "  5.0378262e-06]]\n",
      "Family Finance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Mar/2021 04:36:18] \"\u001b[37mPOST /getfile HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    agent noise customer noise agent silence custo...\n",
      "Name: clean_file_content, dtype: object\n",
      "(1, 1680)\n",
      "[[3.98971402e-04 4.28675594e-06 9.97157574e-01 1.01022783e-03\n",
      "  1.41721207e-03 1.17473855e-05]]\n",
      "Credit Card\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Mar/2021 04:36:38] \"\u001b[37mPOST /getfile HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    agent silence customer silence customer hi mau...\n",
      "Name: clean_file_content, dtype: object\n",
      "(1, 1680)\n",
      "[[1.3621047e-05 5.7914440e-02 1.7494551e-04 9.4169968e-01 1.9197357e-04\n",
      "  5.3414296e-06]]\n",
      "Family Finance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Mar/2021 04:37:37] \"\u001b[37mPOST /getfile HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    agent silence customer noise thoughts company ...\n",
      "Name: clean_file_content, dtype: object\n",
      "(1, 1680)\n",
      "[[1.6496149e-04 1.7250291e-05 1.2383355e-04 8.1595726e-06 9.9968565e-01\n",
      "  7.4723786e-08]]\n",
      "Job Benefits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Mar/2021 04:38:02] \"\u001b[37mPOST /getfile HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    agent noise customer noise agent silence custo...\n",
      "Name: clean_file_content, dtype: object\n",
      "(1, 1680)\n",
      "[[0.00886965 0.4247399  0.00158372 0.00095043 0.00784609 0.5560101 ]]\n",
      "Taxes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Mar/2021 04:38:27] \"\u001b[37mPOST /getfile HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    agent noise customer noise customer well uh kn...\n",
      "Name: clean_file_content, dtype: object\n",
      "(1, 1680)\n",
      "[[3.3886121e-05 7.0710611e-01 1.8271609e-04 2.9222685e-01 1.8318297e-04\n",
      "  2.6717989e-04]]\n",
      "Budget\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Mar/2021 04:38:52] \"\u001b[37mPOST /getfile HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    agent noise topic today customer noise custome...\n",
      "Name: clean_file_content, dtype: object\n",
      "(1, 1680)\n",
      "[[2.84697657e-04 9.66136754e-01 1.00715755e-04 3.23366523e-02\n",
      "  1.07856991e-03 6.27010158e-05]]\n",
      "Budget\n"
     ]
    }
   ],
   "source": [
    "#Flask Code\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from flask import Flask, request, jsonify\n",
    "from flask import Flask, render_template, request\n",
    "from werkzeug import secure_filename\n",
    "import re\n",
    "import flask\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import concatenate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize          \n",
    "from nltk.corpus import stopwords\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Input, Bidirectional,TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "#graph = tf.get_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "# Define the app\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "# Load the model\n",
    "##Attention Layer\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "class AttentionWithContext(Layer):    \n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        \n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "\n",
    "import keras\n",
    "global model_n1\n",
    "global graph\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "model_n1 = keras.models.load_model('my_GRU_model.h5', custom_objects={'AttentionWithContext':AttentionWithContext()})\n",
    "\n",
    "model_n1.compile(Adam(lr=0.01), 'categorical_crossentropy', metrics=['accuracy'])\n",
    "model_n1.load_weights(\"model_doc_tagging_attention_GRU_glove_vec_100d_new.h5\")\n",
    "#model_n1 = keras.models.load_model('my_GRU_model.h5', custom_objects={'AttentionWithContext':AttentionWithContext()})\n",
    "\n",
    "#model_n1.compile(Adam(lr=0.01), 'categorical_crossentropy', metrics=['accuracy'])\n",
    "#model_n1.load_weights(\"model_doc_tagging_attention_GRU_glove_vec_100d_new.h5\")\n",
    "\n",
    "file = open(\"tokenizer.pickle\",'rb')\n",
    "tokenizer = pickle.load(file)\n",
    "print(\"len(tokenizer.word_index) + 1:\",len(tokenizer.word_index) + 1)\n",
    "file.close()\n",
    "\n",
    "# API route\n",
    "@app.route('/getfile', methods=['GET','POST'])\n",
    "def getfile():\n",
    "    if request.method == 'POST':\n",
    "        labels = ['Bank Bailout', 'Budget', 'Credit Card', 'Family Finance', \n",
    "        'Job Benefits', 'Taxes'] \n",
    "        \n",
    "        def cleanSentence(sentence):\n",
    "             sentence_clean= re.sub(\"[^a-zA-Z0-9]\",\" \", str(sentence))\n",
    "             sentence_clean = sentence_clean.lower()\n",
    "             tokens = word_tokenize(sentence_clean)\n",
    "             stop_words = set(stopwords.words(\"english\"))\n",
    "             sentence_clean_words = [w for w in tokens if not w in stop_words]\n",
    "             return ' '.join(sentence_clean_words)\n",
    "\n",
    "        def getTrainSequences(sentence, tokenizer,seq_maxlen):\n",
    "            sent1 = tokenizer.texts_to_sequences(sentence)\n",
    "            #sent_maxlen = max([len(s) for s in sent1])\n",
    "            #seq_maxlen = max([sent_maxlen])\n",
    "            return np.array(pad_sequences(sent1, maxlen=seq_maxlen))\n",
    "        #Need pickel files of Tokenizer,Embedding weight and Glove matrix and freezed model\n",
    "\n",
    "        # for secure filenames. Read the documentation.\n",
    "        file = request.files['file']\n",
    "        file.save(os.path.join('C:\\\\Users\\\\Atchyuta\\\\Downloads\\\\',file.filename))\n",
    "        with open('C:\\\\Users\\\\Atchyuta\\\\Downloads\\\\'+ file.filename) as f:\n",
    "            file_content = f.read().replace('\\n', ' ')\n",
    "        #Lines = file.readlines()\n",
    "        #file_content= file.read().replace('\\n', ' ')\n",
    "        file_content=re.sub(\"\\d+\\.\\d+ \", '',file_content)\n",
    "        #print(\"in:\",file_content)\n",
    "        #data1=pd.DataFrame()\n",
    "        #data1['file_content']=file_content\n",
    "        dict1={\"file_content\":file_content}\n",
    "        data1=pd.DataFrame(dict1,index=[0])\n",
    "        #print(data1['file_content'])\n",
    "        data1['clean_file_content'] = list(map(cleanSentence, data1['file_content']))\n",
    "        print(data1['clean_file_content'])\n",
    "        corpus_textn = '\\n'.join((data1['clean_file_content']))\n",
    "        sentences1 = corpus_textn.split('\\n')\n",
    "        train_data_need1 = getTrainSequences(sentences1, tokenizer,seq_maxlen=1680)\n",
    "        print(train_data_need1.shape)\n",
    "        #model_n1.load_weights(\"model_doc_tagging_attention_GRU_glove_vec_100d_new.h5\")\n",
    "        with graph.as_default():\n",
    "            result=model_n1.predict(train_data_need1)\n",
    "        print(result)\n",
    "        result=result.argmax(axis=-1)\n",
    "        label= labels[result[0]]\n",
    "        print(label)\n",
    "        #filename = secure_filename(file.filename) \n",
    "        test={\"result\":result}\n",
    "        #keras.backend.clear_session()\n",
    "\n",
    "        \"\"\"# os.path.join is used so that paths work in every operating system\n",
    "        file.save(os.path.join(\"wherever\",\"you\",\"want\",filename))\n",
    "\n",
    "        # You should use os.path.join here too.\n",
    "        with open(\"wherever/you/want/filename\") as f:\n",
    "            file_content = f.read()\"\"\"\n",
    "\n",
    "        return flask.jsonify({'predicted_tag': label })  \n",
    "\n",
    "\n",
    "   \n",
    "    return \"txt file needed\"\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return \"Index API\"\n",
    "\n",
    "\n",
    "# HTTP Error handlers\n",
    "@app.errorhandler(404)\n",
    "def url_error(e):\n",
    "    return f\"\"\"Wrong URL! <pre>{e}</pre>\"\"\", 404\n",
    "\n",
    "\n",
    "@app.errorhandler(500)\n",
    "def server_error(e):\n",
    "    return (\n",
    "        f\"\"\"An internal error occured: <pre>{e}</pre>. See logs for full stacktrace\"\"\",\n",
    "        500,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This is used when running locally\n",
    "    app.run(host=\"0.0.0.0\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-kidney",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
